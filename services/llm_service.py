"""
ä¼ä¸šçº§ LLM è·¯ç”±ç½‘å…³ â€” services/llm_service.py
================================================
LLMOps çº§åˆ«çš„"æ¨¡å‹è·¯ç”±ç½‘å…³ (AI Gateway)"ï¼š
  1. AITask åœºæ™¯æšä¸¾            â†’ æŒ‰ä»»åŠ¡ç±»å‹è·¯ç”±åˆ°æœ€ä¼˜æ¨¡å‹
  2. ModelRegistry åŠ¨æ€æ³¨å†Œè¡¨    â†’ å¯é€šè¿‡å‰ç«¯/DB é…ç½®è¦†ç›–
  3. 5 çº§å›é€€é˜²çº¿               â†’ ç²¾å‡†å¼‚å¸¸æ•è·ä¸æ— ç¼é™çº§
  4. AuditLog å®¡è®¡æ—¥å¿—          â†’ è®°å½•æ¯æ¬¡è°ƒç”¨çš„æ¨¡å‹/è€—æ—¶/ç»“æœ

æ³¨æ„ï¼šä¿ç•™åŸç‰ˆ llm_service.py ä¸ºæ—§ç‰ˆå…¼å®¹å±‚ï¼Œæœ¬æ–‡ä»¶ä¸ºæ–°æ¶æ„ã€‚
"""

import enum
import json
import logging
import sys
import time
from dataclasses import dataclass, field
from datetime import datetime, timezone
from typing import Any, Optional

import openai
from openai import OpenAI

logger = logging.getLogger("llm_gateway")
logger.setLevel(logging.DEBUG)

# ANSI é¢œè‰²å¸¸é‡
_YELLOW = "\033[93m"
_RED = "\033[91m"
_GREEN = "\033[92m"
_CYAN = "\033[96m"
_RESET = "\033[0m"
_BOLD = "\033[1m"


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 1. åœºæ™¯åŒ–ä»»åŠ¡æšä¸¾ (Task-Specific Routing)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class AITask(str, enum.Enum):
    """
    AI åœºæ™¯æšä¸¾ â€” ä¸åŒåœºæ™¯è·¯ç”±åˆ°ä¸åŒçš„æ¨¡å‹ç»„åˆã€‚
    æ¯ä¸ªåœºæ™¯æœ‰ç‹¬ç«‹çš„é¦–é€‰æ¨¡å‹ã€æ¸©åº¦ã€æœ€å¤§ tokenã€‚
    """
    FAST_EXTRACT = "fast_extract"
    """å¿«é€Ÿæå– (æƒ…æŠ¥è§£æ/è¯¢æŠ¥ä»·/BOMæå–)
       ç‰¹ç‚¹ï¼šä½å»¶è¿Ÿä¼˜å…ˆï¼Œå…è®¸è½»é‡æ¨¡å‹"""

    HEAVY_STRATEGY = "heavy_strategy"
    """é‡åº¦ç­–ç•¥ (NBAæŠ¥å‘Š/æŠ¤ç›®é•œ/è¯æœ¯ç”Ÿæˆ/æƒåŠ›å›¾è°±)
       ç‰¹ç‚¹ï¼šé«˜è´¨é‡ä¼˜å…ˆï¼Œä½¿ç”¨æœ€å¼ºæ»¡è¡€æ¨¡å‹"""

    VISION_PARSE = "vision_parse"
    """è§†è§‰è§£æ (å›¾ç‰‡æƒ…æŠ¥/åç‰‡/ç°åœºç…§ç‰‡)
       ç‰¹ç‚¹ï¼šå¿…é¡»ä½¿ç”¨å¤šæ¨¡æ€ Vision æ¨¡å‹"""

    CODE_GEN = "code_gen"
    """ä»£ç ç”Ÿæˆ (Mermaidå›¾è°±/æµç¨‹å›¾/JSONç»“æ„åŒ–)
       ç‰¹ç‚¹ï¼šéœ€è¦å¼ºé€»è¾‘æ¨ç†èƒ½åŠ›"""

    QUIZ_CRITIQUE = "quiz_critique"
    """ä¼´å­¦ä¸­å¿ƒ (å‡ºé¢˜/è¯„åˆ†/ç›²ç‚¹åˆ†æ)
       ç‰¹ç‚¹ï¼šä¸­ç­‰å¤æ‚åº¦ï¼Œå¹³è¡¡é€Ÿåº¦ä¸è´¨é‡"""

    SOS_BRIEF = "sos_brief"
    """SOSæ±‚æ´æ‘˜è¦ (ç°åœºç´§æ€¥ï¼Œéœ€æé€Ÿå“åº”)
       ç‰¹ç‚¹ï¼šæœ€ä½å»¶è¿Ÿï¼Œç®€çŸ­å›å¤"""

    GENERAL_CHAT = "general_chat"
    """é€šç”¨å¯¹è¯ (å†›å¸ˆå¯¹è¯/å‘¨æŠ¥/å¤ç›˜)"""


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 2. æ¨¡å‹æä¾›å•†é…ç½®
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

@dataclass
class LLMProvider:
    """å•ä¸ª LLM æä¾›å•†é…ç½®ã€‚"""
    name: str           # "OpenAI" / "Google Gemini" / "Anthropic" / "xAI" / "Local"
    model: str          # å…·ä½“æ¨¡å‹ç‰ˆæœ¬å·
    base_url: str       # API endpoint
    api_key: str        # åŠ¨æ€ä¼ å…¥
    timeout: int = 30   # è¶…æ—¶ç§’æ•°
    supports_vision: bool = False  # æ˜¯å¦æ”¯æŒå¤šæ¨¡æ€


@dataclass
class AuditEntry:
    """æ¨¡å‹è°ƒç”¨å®¡è®¡æ—¥å¿—æ¡ç›®ã€‚"""
    task: str
    provider: str
    model: str
    success: bool
    latency_ms: int
    error: Optional[str] = None
    timestamp: str = ""

    def __post_init__(self):
        if not self.timestamp:
            self.timestamp = datetime.now(timezone.utc).isoformat()


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 3. åŠ¨æ€æ¨¡å‹æ³¨å†Œè¡¨ (ModelRegistry)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# é»˜è®¤çš„ åœºæ™¯ â†’ (é¦–é€‰æ¨¡å‹ç‰ˆæœ¬, å›é€€æ¨¡å‹ç‰ˆæœ¬, æ¸©åº¦, max_tokens) æ˜ å°„
DEFAULT_MODEL_REGISTRY: dict[AITask, dict] = {
    AITask.FAST_EXTRACT: {
        "openai": "gpt-4o-mini",
        "gemini": "gemini-2.0-flash",
        "anthropic": "claude-3-5-haiku-20241022",
        "xai": "grok-3-mini",
        "local": "deepseek-r1",
        "temperature": 0.1,
        "max_tokens": 4096,
    },
    AITask.HEAVY_STRATEGY: {
        "openai": "gpt-4o",
        "gemini": "gemini-2.5-pro-preview-05-06",
        "anthropic": "claude-sonnet-4-20250514",
        "xai": "grok-3",
        "local": "deepseek-r1",
        "temperature": 0.6,
        "max_tokens": 8192,
    },
    AITask.VISION_PARSE: {
        "openai": "gpt-4o",
        "gemini": "gemini-2.0-flash",
        "anthropic": "claude-sonnet-4-20250514",
        "xai": "grok-3",
        "local": "deepseek-r1",
        "temperature": 0.2,
        "max_tokens": 4096,
    },
    AITask.CODE_GEN: {
        "openai": "gpt-4o",
        "gemini": "gemini-2.5-pro-preview-05-06",
        "anthropic": "claude-sonnet-4-20250514",
        "xai": "grok-3",
        "local": "deepseek-r1",
        "temperature": 0.3,
        "max_tokens": 4096,
    },
    AITask.QUIZ_CRITIQUE: {
        "openai": "gpt-4o-mini",
        "gemini": "gemini-2.0-flash",
        "anthropic": "claude-3-5-haiku-20241022",
        "xai": "grok-3-mini",
        "local": "deepseek-r1",
        "temperature": 0.5,
        "max_tokens": 4096,
    },
    AITask.SOS_BRIEF: {
        "openai": "gpt-4o-mini",
        "gemini": "gemini-2.0-flash",
        "anthropic": "claude-3-5-haiku-20241022",
        "xai": "grok-3-mini",
        "local": "deepseek-r1",
        "temperature": 0.7,
        "max_tokens": 2048,
    },
    AITask.GENERAL_CHAT: {
        "openai": "gpt-4o",
        "gemini": "gemini-2.0-flash",
        "anthropic": "claude-sonnet-4-20250514",
        "xai": "grok-3",
        "local": "deepseek-r1",
        "temperature": 0.6,
        "max_tokens": 4096,
    },
}

# Provider åç§° â†’ æ³¨å†Œè¡¨ key çš„æ˜ å°„
_PROVIDER_KEY_MAP = {
    "OpenAI": "openai",
    "Google Gemini": "gemini",
    "Anthropic": "anthropic",
    "xAI Grok": "xai",
    "Local DeepSeek": "local",
}


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 4. ä¼ä¸šçº§å…¨å±€è·¯ç”±å™¨
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class AIGateway:
    """
    ä¼ä¸šçº§ LLM è·¯ç”±ç½‘å…³ (AI Gateway)
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    æ ¸å¿ƒèƒ½åŠ›ï¼š
    1. åœºæ™¯è‡ªé€‚åº”è·¯ç”±  â€” ä¸åŒ AITask è‡ªåŠ¨é€‰æ‹©æœ€ä¼˜æ¨¡å‹ç‰ˆæœ¬
    2. 5 çº§å›é€€é˜²çº¿    â€” OpenAI â†’ Gemini â†’ Anthropic â†’ xAI â†’ Local
    3. åŠ¨æ€é…ç½®è¦†ç›–    â€” å‰ç«¯/DB ä¼ å…¥ model_overrides å¯è¦†ç›–é»˜è®¤é€‰æ‹©
    4. å®¡è®¡æ—¥å¿—        â€” æ¯æ¬¡è°ƒç”¨è®°å½• provider/model/å»¶è¿Ÿ/æˆè´¥
    """

    def __init__(
        self,
        providers: list[LLMProvider],
        model_registry: dict[AITask, dict] | None = None,
    ):
        self.providers = providers
        self.registry = model_registry or DEFAULT_MODEL_REGISTRY.copy()
        self.audit_log: list[AuditEntry] = []

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # æ ¸å¿ƒï¼šåœºæ™¯æ„ŸçŸ¥çš„æ™ºèƒ½è°ƒç”¨
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    def chat(
        self,
        messages: list[dict],
        task: AITask = AITask.GENERAL_CHAT,
        temperature: float | None = None,
        model_overrides: dict | None = None,
        **kwargs,
    ) -> str:
        """
        ç»Ÿä¸€è°ƒç”¨å…¥å£ â€” åœºæ™¯æ„ŸçŸ¥ + è‡ªåŠ¨å›é€€ã€‚

        Args:
            messages:        æ ‡å‡† OpenAI messages æ ¼å¼
            task:            AI åœºæ™¯æšä¸¾ï¼Œå†³å®šé¦–é€‰æ¨¡å‹
            temperature:     è¦†ç›–é»˜è®¤æ¸©åº¦ (None = ä½¿ç”¨æ³¨å†Œè¡¨é»˜è®¤)
            model_overrides: åŠ¨æ€è¦†ç›– {"openai": "gpt-4o", "gemini": "..."}
                             å‰ç«¯è®¾ç½®é¡µé¢æˆ– DB é…ç½®å¯ä¼ å…¥

        Returns:
            AI ç”Ÿæˆçš„æ–‡æœ¬å†…å®¹

        Raises:
            RuntimeError: å…¨éƒ¨é˜²çº¿å¤±è´¥
        """
        # è¯»å–è¯¥åœºæ™¯çš„æ³¨å†Œè¡¨é…ç½®
        task_config = self.registry.get(task, self.registry[AITask.GENERAL_CHAT])
        temp = temperature if temperature is not None else task_config.get("temperature", 0.6)

        # åˆå¹¶åŠ¨æ€è¦†ç›–ï¼ˆå‰ç«¯/DB é…ç½® > æ³¨å†Œè¡¨é»˜è®¤ï¼‰
        overrides = model_overrides or {}

        errors: list[str] = []
        active_providers = [p for p in self.providers if p.api_key]
        total = len(active_providers)

        for idx, provider in enumerate(active_providers, 1):
            # æ ¹æ®åœºæ™¯ + è¦†ç›–ç¡®å®šè¯¥ provider ä½¿ç”¨çš„æ¨¡å‹ç‰ˆæœ¬
            provider_key = _PROVIDER_KEY_MAP.get(provider.name, "openai")
            model = (
                overrides.get(provider_key)                      # ä¼˜å…ˆï¼šåŠ¨æ€è¦†ç›–
                or task_config.get(provider_key)                 # å…¶æ¬¡ï¼šæ³¨å†Œè¡¨åœºæ™¯é…ç½®
                or provider.model                                # å…œåº•ï¼šprovider é»˜è®¤
            )

            start_time = time.monotonic()

            try:
                print(
                    f"{_CYAN}{_BOLD}ğŸ”— [{idx}/{total}] "
                    f"[{task.value}] å°è¯• {provider.name} ({model})...{_RESET}",
                    file=sys.stderr,
                )

                content = self._call_provider(provider, model, messages, temp)

                elapsed_ms = int((time.monotonic() - start_time) * 1000)
                print(
                    f"{_GREEN}{_BOLD}âœ… {provider.name} ({model}) "
                    f"å‘½ä¸­æˆåŠŸï¼è€—æ—¶ {elapsed_ms}ms{_RESET}",
                    file=sys.stderr,
                )

                # å®¡è®¡æ—¥å¿—
                self.audit_log.append(AuditEntry(
                    task=task.value, provider=provider.name,
                    model=model, success=True, latency_ms=elapsed_ms,
                ))

                return content

            except openai.AuthenticationError as e:
                msg = f"[{provider.name}] ğŸ”‘ AuthError (401): Key æ— æ•ˆ"
                errors.append(msg)
                self._log_fallback(provider, model, "AuthError", e, start_time, task)
                continue

            except openai.RateLimitError as e:
                msg = f"[{provider.name}] ğŸš¦ RateLimit (429): è§¦å‘é™æµ"
                errors.append(msg)
                self._log_fallback(provider, model, "RateLimit", e, start_time, task)
                continue

            except openai.APITimeoutError as e:
                msg = f"[{provider.name}] â±ï¸ Timeout ({provider.timeout}s)"
                errors.append(msg)
                self._log_fallback(provider, model, "Timeout", e, start_time, task)
                continue

            except (openai.APIConnectionError, openai.InternalServerError) as e:
                msg = f"[{provider.name}] ğŸ’¥ {type(e).__name__}: æœåŠ¡å¼‚å¸¸"
                errors.append(msg)
                self._log_fallback(provider, model, type(e).__name__, e, start_time, task)
                continue

            except openai.BadRequestError as e:
                msg = f"[{provider.name}] âš ï¸ BadRequest (400): {e}"
                errors.append(msg)
                self._log_fallback(provider, model, "BadRequest", e, start_time, task)
                continue

            except Exception as e:
                msg = f"[{provider.name}] â“ {type(e).__name__}: {e}"
                errors.append(msg)
                self._log_fallback(provider, model, type(e).__name__, e, start_time, task)
                continue

        # å…¨éƒ¨å¤±è´¥
        error_detail = "\n".join(errors)
        print(
            f"{_RED}{_BOLD}ğŸš¨ æ‰€æœ‰ LLM é˜²çº¿å‡å·²å¤±è´¥ (task={task.value})ï¼"
            f"\n{error_detail}{_RESET}",
            file=sys.stderr,
        )
        raise RuntimeError(f"æ‰€æœ‰ LLM é˜²çº¿å‡å·²å¤±è´¥:\n{error_detail}")

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # å†…éƒ¨ï¼šå®é™…è°ƒç”¨ Provider
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    def _call_provider(
        self,
        provider: LLMProvider,
        model: str,
        messages: list[dict],
        temperature: float,
    ) -> str:
        """
        å®é™…è°ƒç”¨ LLM Providerã€‚
        Anthropic ä½¿ç”¨åŸç”Ÿ SDKï¼Œå…¶ä½™èµ° OpenAI å…¼å®¹å±‚ã€‚
        """
        if provider.name == "Anthropic":
            return self._call_anthropic(provider, model, messages, temperature)
        else:
            client = OpenAI(
                api_key=provider.api_key,
                base_url=provider.base_url,
                timeout=provider.timeout,
            )
            response = client.chat.completions.create(
                model=model,
                messages=messages,
                temperature=temperature,
            )
            return response.choices[0].message.content

    def _call_anthropic(
        self,
        provider: LLMProvider,
        model: str,
        messages: list[dict],
        temperature: float,
    ) -> str:
        """Anthropic åŸç”Ÿ SDK è°ƒç”¨ï¼ˆæ¶ˆæ¯æ ¼å¼è½¬æ¢ï¼‰ã€‚"""
        import anthropic
        client = anthropic.Anthropic(
            api_key=provider.api_key,
            timeout=provider.timeout,
        )
        system_text = ""
        user_msgs = []
        for m in messages:
            if m["role"] == "system":
                system_text += m["content"] + "\n"
            else:
                user_msgs.append({"role": m["role"], "content": m["content"]})
        if not user_msgs:
            user_msgs = [{"role": "user", "content": system_text}]
            system_text = ""

        create_kwargs = {
            "model": model,
            "max_tokens": 4096,
            "messages": user_msgs,
            "temperature": temperature,
        }
        if system_text.strip():
            create_kwargs["system"] = system_text.strip()

        response = client.messages.create(**create_kwargs)
        return response.content[0].text

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # å†…éƒ¨ï¼šå›é€€æ—¥å¿—
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    def _log_fallback(
        self,
        provider: LLMProvider,
        model: str,
        error_type: str,
        error: Exception,
        start_time: float,
        task: AITask,
    ):
        elapsed_ms = int((time.monotonic() - start_time) * 1000)
        print(
            f"{_YELLOW}{_BOLD}âš ï¸ {provider.name} ({model}) "
            f"{error_type}ï¼Œè€—æ—¶ {elapsed_ms}msï¼Œ"
            f"åˆ‡æ¢è‡³ä¸‹ä¸€é˜²çº¿...{_RESET}",
            file=sys.stderr,
        )
        logger.warning(
            "LLM fallback | task=%s provider=%s model=%s error=%s latency=%dms",
            task.value, provider.name, model, error_type, elapsed_ms,
        )
        self.audit_log.append(AuditEntry(
            task=task.value, provider=provider.name,
            model=model, success=False, latency_ms=elapsed_ms,
            error=f"{error_type}: {str(error)[:200]}",
        ))

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # å®¡è®¡æ—¥å¿—æŸ¥è¯¢
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    def get_audit_log(self, last_n: int = 50) -> list[dict]:
        """è·å–æœ€è¿‘ N æ¡å®¡è®¡æ—¥å¿—ã€‚"""
        entries = self.audit_log[-last_n:]
        return [
            {
                "task": e.task,
                "provider": e.provider,
                "model": e.model,
                "success": e.success,
                "latency_ms": e.latency_ms,
                "error": e.error,
                "timestamp": e.timestamp,
            }
            for e in entries
        ]

    def get_stats(self) -> dict:
        """è·å–è°ƒç”¨ç»Ÿè®¡æ‘˜è¦ã€‚"""
        total = len(self.audit_log)
        success = sum(1 for e in self.audit_log if e.success)
        by_provider: dict[str, dict] = {}
        for e in self.audit_log:
            if e.provider not in by_provider:
                by_provider[e.provider] = {"total": 0, "success": 0, "avg_ms": 0, "latencies": []}
            by_provider[e.provider]["total"] += 1
            if e.success:
                by_provider[e.provider]["success"] += 1
            by_provider[e.provider]["latencies"].append(e.latency_ms)

        for stats in by_provider.values():
            lats = stats.pop("latencies")
            stats["avg_ms"] = int(sum(lats) / len(lats)) if lats else 0

        return {
            "total_calls": total,
            "success_rate": f"{(success / total * 100):.1f}%" if total > 0 else "N/A",
            "by_provider": by_provider,
        }


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 5. ç½‘å…³å·¥å‚å‡½æ•°
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def build_ai_gateway(
    primary_api_key: str = "",
    llm_configs: dict | None = None,
    model_overrides: dict[str, dict] | None = None,
) -> AIGateway:
    """
    æ„å»º AIGateway å®ä¾‹ã€‚

    Args:
        primary_api_key: ä¸» API Key (å‘åå…¼å®¹)
        llm_configs:     å‰ç«¯è®¾ç½®çš„å®Œæ•´ LLM é…ç½® (å«å„ provider çš„ key/model/enabled)
        model_overrides: åœºæ™¯çº§æ¨¡å‹è¦†ç›– {"fast_extract": {"openai": "gpt-4o"}, ...}

    Returns:
        AIGateway å®ä¾‹
    """
    cfg = llm_configs or {}

    def _get(provider_key: str, field_name: str, default: str) -> str:
        return cfg.get(provider_key, {}).get(field_name, "") or default

    def _enabled(provider_key: str, default: bool) -> bool:
        p = cfg.get(provider_key, {})
        if isinstance(p, dict) and "enabled" in p:
            return bool(p["enabled"])
        return default

    providers: list[LLMProvider] = []

    # â”€â”€ ç¬¬ä¸€é˜²çº¿: OpenAI â”€â”€
    if _enabled("openai", True):
        key = _get("openai", "apiKey", primary_api_key)
        if key:
            providers.append(LLMProvider(
                name="OpenAI",
                model=_get("openai", "model", "gpt-4o-mini"),
                base_url=_get("openai", "baseUrl", "https://api.openai.com/v1"),
                api_key=key,
                timeout=30,
                supports_vision=True,
            ))

    # â”€â”€ ç¬¬äºŒé˜²çº¿: Google Gemini â”€â”€
    if _enabled("gemini", False):
        key = _get("gemini", "apiKey", "")
        if key:
            providers.append(LLMProvider(
                name="Google Gemini",
                model=_get("gemini", "model", "gemini-2.0-flash"),
                base_url=_get("gemini", "baseUrl",
                              "https://generativelanguage.googleapis.com/v1beta/openai/"),
                api_key=key,
                timeout=30,
                supports_vision=True,
            ))

    # â”€â”€ ç¬¬ä¸‰é˜²çº¿: Anthropic â”€â”€
    if _enabled("anthropic", False):
        key = _get("anthropic", "apiKey", "")
        if key:
            providers.append(LLMProvider(
                name="Anthropic",
                model=_get("anthropic", "model", "claude-sonnet-4-20250514"),
                base_url=_get("anthropic", "baseUrl", "https://api.anthropic.com/v1/"),
                api_key=key,
                timeout=45,
                supports_vision=True,
            ))

    # â”€â”€ ç¬¬å››é˜²çº¿: xAI Grok â”€â”€
    if _enabled("xai", False):
        key = _get("xai", "apiKey", "")
        if key:
            providers.append(LLMProvider(
                name="xAI Grok",
                model=_get("xai", "model", "grok-3-mini"),
                base_url=_get("xai", "baseUrl", "https://api.x.ai/v1"),
                api_key=key,
                timeout=30,
                supports_vision=False,
            ))

    # â”€â”€ ç¬¬äº”é˜²çº¿: Local DeepSeek â”€â”€
    if _enabled("local", bool(not providers)):
        # ä»…åœ¨æ— äº‘ç«¯ provider æ—¶é»˜è®¤å¯ç”¨æœ¬åœ°
        providers.append(LLMProvider(
            name="Local DeepSeek",
            model=_get("local", "model", "deepseek-r1"),
            base_url=_get("local", "baseUrl", "http://localhost:11434/v1"),
            api_key="local",
            timeout=120,
            supports_vision=False,
        ))

    # æ„å»ºæ³¨å†Œè¡¨ (åˆå¹¶åœºæ™¯çº§è¦†ç›–)
    registry = DEFAULT_MODEL_REGISTRY.copy()
    if model_overrides:
        for task_str, overrides in model_overrides.items():
            try:
                task_enum = AITask(task_str)
                if task_enum in registry:
                    registry[task_enum] = {**registry[task_enum], **overrides}
            except ValueError:
                pass  # å¿½ç•¥æœªçŸ¥åœºæ™¯

    return AIGateway(providers=providers, model_registry=registry)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 6. å‘åå…¼å®¹å±‚ (ä¿æŒæ—§ç‰ˆ API ä¸ä¸­æ–­)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# æ—§ç‰ˆåˆ«å â€” ç¡®ä¿å·²æœ‰ routers/api.py ä¸­çš„ build_llm_router è°ƒç”¨ä¸ä¼šå´©æºƒ
GlobalLLMRouter = AIGateway

def build_llm_router(
    primary_api_key: str = "",
    llm_configs: dict | None = None,
) -> AIGateway:
    """å‘åå…¼å®¹æ—§ç‰ˆ build_llm_router() è°ƒç”¨ã€‚"""
    return build_ai_gateway(primary_api_key=primary_api_key, llm_configs=llm_configs)


def _detect_llm_config(api_key: str) -> dict:
    """æ ¹æ® API Key å‰ç¼€è‡ªåŠ¨æ£€æµ‹ LLM æä¾›å•†ã€‚"""
    if api_key.startswith("sk-ant-"):
        return {
            "openai": {"enabled": False},
            "anthropic": {"enabled": True, "apiKey": api_key,
                          "model": "claude-sonnet-4-20250514"},
            "local": {"enabled": False},
        }
    return {}
