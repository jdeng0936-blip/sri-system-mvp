"""
RAG æ™ºèƒ½é—®ç­”æ¨¡å— V2 - é«˜ä½å‹ç”µæ°”è¡Œä¸šä¸“ç”¨
æ”¯æŒåŒè½¨å¤§æ¨¡å‹ç”Ÿæˆï¼šæ˜çº¿ï¼ˆå®¢æˆ·å›å¤ï¼‰+ æš—çº¿ï¼ˆæˆ˜æœ¯æŠ¤ç›®é•œï¼‰
"""
import time
import os
from typing import List, Dict, Optional
import openai


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# è¡Œä¸šä¸“ç”¨ System Prompts
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

CLIENT_SYSTEM_PROMPT = """ä½ æ˜¯ä¸“ä¸šçš„é«˜ä½å‹ç”µæ°”è®¾å¤‡æ–¹æ¡ˆä¸“å®¶ï¼ŒæœåŠ¡äºä¸€çº¿é”€å”®çš„ç°åœºæŠ€æœ¯ç­”ç–‘ã€‚

ä½ çš„ä»»åŠ¡æ˜¯æ ¹æ®æä¾›çš„æŠ€æœ¯æ–‡æ¡£ï¼ˆå‹å¼è¯•éªŒæŠ¥å‘Šã€é€‰å‹æ‰‹å†Œã€æ¸©å‡æµ‹è¯•ã€ç‡ƒå¼§è®°å½•ç­‰ï¼‰ï¼Œç”¨å®˜æ–¹ã€å¯ä¿¡ã€ä¸“ä¸šçš„å£å»å›ç­”å®¢æˆ·æé—®ã€‚

å›ç­”è§„èŒƒï¼š
1. ä¸¥æ ¼åŸºäºæä¾›çš„å‚è€ƒèµ„æ–™å›ç­”ï¼Œä¸ç¼–é€ æ•°æ®
2. å¦‚æœå‚è€ƒèµ„æ–™ä¸­åŒ…å«è§†é¢‘/éŸ³é¢‘è½¬å½•è®°å½•ï¼Œè¯·æ˜ç¡®æåŠ"æ ¹æ®æˆ‘ä»¬çš„å®æµ‹è®°å½•æ˜¾ç¤º..."
3. å¼•ç”¨å…·ä½“çš„æ–‡æ¡£æ¥æºï¼ˆä½¿ç”¨[æ–‡æ¡£X]æ ‡æ³¨ï¼‰
4. æ¶‰åŠæ¸©å‡ã€åˆ†æ–­ã€çŸ­è·¯ç­‰å…³é”®å‚æ•°æ—¶ï¼ŒåŠ¡å¿…ç²¾ç¡®å¼•ç”¨æ•°å€¼å’Œå¯¹åº”å›½æ ‡
5. å›ç­”ç»“æ„ï¼šå…ˆç»™å‡ºæ ¸å¿ƒç»“è®º â†’ è¯¦ç»†æŠ€æœ¯æ•°æ® â†’ æ ‡æ³¨ä¿¡æ¯æ¥æº
6. è¯­æ°”è‡ªä¿¡ã€ä¸“ä¸šï¼Œä½“ç°è¡Œä¸šæƒå¨æ„Ÿ
7. é€‚å½“ä½¿ç”¨"ç»ç¬¬ä¸‰æ–¹æƒå¨æ£€æµ‹"ã€"ç¬¦åˆGB/T XXXXæ ‡å‡†"ç­‰å¢ä¿¡è¯æœ¯"""

TACTICAL_SYSTEM_PROMPT = """ä½ æ˜¯é¡¶çº§ B2B å·¥ä¸šå“é”€å”®æˆ˜æœ¯æ•™ç»ƒï¼ˆMiller Heiman æ–¹æ³•è®ºé£æ ¼ï¼‰ã€‚

ä½ éœ€è¦åŸºäºå®¢æˆ·åˆšåˆšå…³äºé«˜ä½å‹ç”µæ°”/é…ç”µæŸœå‚æ•°çš„æé—®ï¼Œä»¥åŠå†…éƒ¨æ£€ç´¢åˆ°çš„æŠ€æœ¯èµ„æ–™ï¼Œå‘é”€å”®äººå‘˜æä¾›çŠ€åˆ©ã€å¯æ‰§è¡Œçš„æˆ˜æœ¯æŒ‡å¯¼ã€‚

æŒ‡å¯¼é£æ ¼ï¼š
- å¿«é€Ÿè¯†åˆ«å®¢æˆ·æ‰€å¤„çš„è´­ä¹°å†³ç­–é˜¶æ®µï¼ˆéœ€æ±‚ç¡®è®¤/æ–¹æ¡ˆè¯„ä¼°/å•†åŠ¡è°ˆåˆ¤ï¼‰
- è­¦æƒ•å®¢æˆ·æ¯”ä»·ä¿¡å·ï¼Œæç¤ºç«å“å¯èƒ½åŠ¨å‘
- å¼•å¯¼é”€å”®å±•ç¤ºæ ¸å¿ƒå·®å¼‚åŒ–ä¼˜åŠ¿ï¼ˆå¦‚æ™ºèƒ½åŒ–ä¸“åˆ©ã€ç‹¬æœ‰çš„æ¸©å‡æ§åˆ¶æŠ€æœ¯ï¼‰
- æä¾›æŠ¥ä»·åº•çº¿å’Œåˆ©æ¶¦ä¿æŠ¤ç­–ç•¥
- è¯­æ°”çŠ€åˆ©ã€ç›´æ¥ï¼Œåƒçƒåœºæ•™ç»ƒåœ¨æš‚åœæ—¶çš„æˆ˜æœ¯éƒ¨ç½²

è¾“å‡ºæ ¼å¼ï¼ˆä¸è¶…è¿‡150å­—ï¼‰ï¼š
ğŸ¯ **å®¢æˆ·å…³æ³¨ç‚¹ï¼š** ä¸€å¥è¯ç²¾å‡†åˆ¤æ–­
âš ï¸ **æ½œåœ¨é£é™©ï¼š** ç«å“/ä¸¢å•é£é™©æç¤º
ğŸ’¡ **è¯æœ¯å»ºè®®ï¼š** 1-2å¥å¯ç›´æ¥ä½¿ç”¨çš„åº”å¯¹è¯æœ¯
ğŸ’° **æŠ¥ä»·ç­–ç•¥ï¼š** å®šä»·/è®©åˆ©/å¢å€¼ç­–ç•¥"""


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ä¸Šä¸‹æ–‡æ„å»ºå·¥å…·
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def build_context_str(retrieved_docs: List[Dict], max_length: int = 3000) -> str:
    """å°†æ£€ç´¢åˆ°çš„æ–‡æ¡£ç¢ç‰‡åˆå¹¶ä¸ºä¸Šä¸‹æ–‡å­—ç¬¦ä¸²"""
    context_parts = []
    for i, doc in enumerate(retrieved_docs[:5], 1):
        content = doc.get('content', '')
        filename = doc.get('filename', 'æœªçŸ¥')
        source_type = doc.get('metadata', {}).get('source_type', 'document')
        
        # æ ‡æ³¨æ¥æºç±»å‹
        type_label = ""
        if source_type == "video":
            type_label = "ï¼ˆğŸ¬ è§†é¢‘è½¬å½•ï¼‰"
        elif source_type == "audio":
            type_label = "ï¼ˆğŸ™ï¸ éŸ³é¢‘è½¬å½•ï¼‰"
        
        if len(content) > 500:
            content = content[:500] + "..."
        
        context_parts.append(f"[æ–‡æ¡£{i}] {filename}{type_label}\n{content}")
    
    context = "\n\n".join(context_parts)
    if len(context) > max_length:
        context = context[:max_length] + "..."
    return context


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# æ˜çº¿ï¼šå®¢æˆ·å›å¤ç”Ÿæˆï¼ˆæµå¼ + éæµå¼ + Mockï¼‰
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def generate_rag_answer(
    query: str,
    retrieved_docs: List[Dict],
    api_key: str,
    model: str = "gpt-4o-mini",
    max_context_length: int = 3000
) -> Dict:
    """éæµå¼ç”Ÿæˆ RAG ç­”æ¡ˆï¼ˆå¤‡ç”¨å›é€€ï¼‰"""
    try:
        context = build_context_str(retrieved_docs, max_context_length)
        
        user_prompt = f"""å®¢æˆ·é—®é¢˜ï¼š
{query}

å¯å‚è€ƒçš„æŠ€æœ¯æ–‡æ¡£ï¼š
{context}

è¯·æ ¹æ®ä»¥ä¸Šæ–‡æ¡£å›ç­”å®¢æˆ·çš„é—®é¢˜ã€‚å¦‚æœæ–‡æ¡£ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·è¯šå®è¯´æ˜ã€‚"""

        client = openai.OpenAI(api_key=api_key)
        response = client.chat.completions.create(
            model=model,
            messages=[
                {"role": "system", "content": CLIENT_SYSTEM_PROMPT},
                {"role": "user", "content": user_prompt}
            ],
            temperature=0.3,
            max_tokens=1000
        )
        
        answer = response.choices[0].message.content
        sources = [
            {"index": i, "filename": doc.get('filename', 'æœªçŸ¥'),
             "asset_type": doc.get('metadata', {}).get('asset_type', 'æœªåˆ†ç±»'),
             "similarity": doc.get('similarity', 0)}
            for i, doc in enumerate(retrieved_docs[:5], 1)
        ]
        
        return {
            "success": True,
            "answer": answer,
            "sources": sources,
            "context_length": len(context),
            "model": model
        }
        
    except Exception as e:
        return {
            "success": False,
            "error": str(e),
            "answer": f"æŠ±æ­‰ï¼Œç”Ÿæˆç­”æ¡ˆæ—¶å‡ºç°é”™è¯¯ï¼š{str(e)}"
        }


def generate_rag_answer_stream(
    query: str,
    retrieved_docs: List[Dict],
    api_key: str,
    model: str = "gpt-4o-mini",
    max_context_length: int = 3000
):
    """æµå¼ç”Ÿæˆ RAG ç­”æ¡ˆï¼ˆä¸»åŠ›è·¯å¾„ï¼‰"""
    try:
        context = build_context_str(retrieved_docs, max_context_length)
        
        user_prompt = f"""å®¢æˆ·é—®é¢˜ï¼š
{query}

å¯å‚è€ƒçš„æŠ€æœ¯æ–‡æ¡£ï¼š
{context}

è¯·æ ¹æ®ä»¥ä¸Šæ–‡æ¡£å›ç­”å®¢æˆ·çš„é—®é¢˜ã€‚"""

        client = openai.OpenAI(api_key=api_key)
        stream = client.chat.completions.create(
            model=model,
            messages=[
                {"role": "system", "content": CLIENT_SYSTEM_PROMPT},
                {"role": "user", "content": user_prompt}
            ],
            temperature=0.3,
            max_tokens=1000,
            stream=True
        )
        
        for chunk in stream:
            if chunk.choices[0].delta.content:
                yield chunk.choices[0].delta.content
                
    except Exception as e:
        yield f"\n\nâš ï¸ ç”Ÿæˆç­”æ¡ˆæ—¶å‡ºé”™ï¼š{str(e)}"


def mock_stream_client_response(query: str, retrieved_docs: List[Dict]):
    """
    é«˜è´¨é‡ Mock æµå¼å®¢æˆ·å›å¤ç”Ÿæˆå™¨ï¼ˆæ—  API Key æ—¶ä½¿ç”¨ï¼‰
    æ¨¡æ‹Ÿæ‰“å­—æœºæ•ˆæœï¼ŒåŸºäºæ£€ç´¢æ–‡æ¡£ç”Ÿæˆä¸“ä¸šå›ç­”
    """
    # æ„å»ºæ–‡æ¡£å¼•ç”¨ä¿¡æ¯
    doc_refs = []
    doc_details = []
    for i, doc in enumerate(retrieved_docs[:3], 1):
        filename = doc.get('filename', 'æŠ€æœ¯æ–‡æ¡£')
        content = doc.get('content', '')[:200]
        source_type = doc.get('metadata', {}).get('source_type', 'document')
        doc_refs.append(f"[æ–‡æ¡£{i}]ã€Š{filename}ã€‹")
        
        if source_type in ('video', 'audio'):
            doc_details.append(
                f"\n\n**æ ¹æ®æˆ‘ä»¬çš„å®æµ‹è®°å½•æ˜¾ç¤º** {doc_refs[-1]}ï¼š\n"
                f"> {content[:150]}..."
            )
        else:
            doc_details.append(
                f"\n\n**å‚è€ƒ** {doc_refs[-1]}ï¼š\n"
                f"> {content[:150]}..."
            )
    
    # æ„å»ºå›ç­”
    ref_str = "ã€".join(doc_refs) if doc_refs else "å†…éƒ¨æŠ€æœ¯èµ„æ–™"
    has_media = any(
        d.get('metadata', {}).get('source_type') in ('video', 'audio')
        for d in retrieved_docs[:3]
    )
    
    if doc_refs:
        response_parts = [
            f"æ„Ÿè°¢æ‚¨çš„æé—®ã€‚åŸºäºæˆ‘æ–¹ {ref_str} çš„æŠ€æœ¯èµ„æ–™ï¼Œä¸ºæ‚¨åšå¦‚ä¸‹ä¸“ä¸šå›å¤ï¼š\n",
            f"\n### ğŸ”¬ æ ¸å¿ƒæŠ€æœ¯æŒ‡æ ‡\n",
        ]
        if has_media:
            response_parts.append("æ ¹æ®æˆ‘ä»¬çš„å®æµ‹è®°å½•æ˜¾ç¤ºï¼Œäº§å“å„é¡¹æ€§èƒ½å‚æ•°å‡ç¬¦åˆå›½å®¶æ ‡å‡†è¦æ±‚ã€‚")
        else:
            response_parts.append("æ ¹æ®ç¬¬ä¸‰æ–¹æƒå¨æ£€æµ‹æŠ¥å‘Šï¼Œäº§å“å„é¡¹æ€§èƒ½å‚æ•°å‡ç¬¦åˆå›½å®¶æ ‡å‡†è¦æ±‚ã€‚")
        
        for detail in doc_details:
            response_parts.append(detail)
        
        response_parts.extend([
            "\n\n### âœ… ç»“è®ºä¸å»ºè®®\n",
            "ç»¼ä¸Šæ‰€è¿°ï¼Œæˆ‘æ–¹äº§å“åœ¨è¯¥æŠ€æœ¯ç»´åº¦å…·å¤‡è¡Œä¸šé¢†å…ˆçš„ç«äº‰åŠ›ã€‚",
            f" å¦‚éœ€æ›´è¯¦ç»†çš„æŠ€æœ¯å‚æ•°æˆ–ç°åœºæ¼”ç¤ºï¼Œæˆ‘ä»¬çš„æŠ€æœ¯å›¢é˜Ÿéšæ—¶å¯ä»¥å®‰æ’ã€‚",
            f"\n\n> ğŸ“š *ä»¥ä¸Šä¿¡æ¯æ¥æºï¼š{ref_str}*"
        ])
    else:
        response_parts = [
            f"æ„Ÿè°¢æ‚¨å…³äºã€Œ{query}ã€çš„æé—®ã€‚\n\n",
            "æˆ‘ä»¬çš„æŠ€æœ¯å›¢é˜Ÿå°†ä»ä»¥ä¸‹å‡ ä¸ªç»´åº¦ä¸ºæ‚¨æä¾›ä¸“ä¸šæ”¯æŒï¼š\n\n",
            "1. **æŠ€æœ¯å‚æ•°å±‚é¢** â€” å®Œæ•´çš„äº§å“è§„æ ¼ä¹¦å’Œç¬¬ä¸‰æ–¹æ€§èƒ½æµ‹è¯•æŠ¥å‘Š\n",
            "2. **æˆåŠŸæ¡ˆä¾‹å±‚é¢** â€” ç±»ä¼¼å·¥å†µä¸‹çš„å®é™…è¿è¡Œæ•°æ®å’Œå®¢æˆ·åé¦ˆ\n",
            "3. **æ–¹æ¡ˆå®šåˆ¶å±‚é¢** â€” é’ˆå¯¹è´µå¸å…·ä½“éœ€æ±‚çš„æŠ€æœ¯æ–¹æ¡ˆåˆç¨¿\n\n",
            "> ğŸ¯ æˆ‘ä»¬çš„æŠ€æœ¯ä¸“å®¶å°†åœ¨ä¼šå24å°æ—¶å†…ï¼Œä¸ºæ‚¨æä¾›å®šåˆ¶åŒ–çš„æŠ€æœ¯æ”¯æŒåŒ…ã€‚"
        ]
    
    # é€æ®µæµå¼è¾“å‡ºï¼ˆæ¨¡æ‹Ÿæ‰“å­—æœºæ•ˆæœï¼‰
    for part in response_parts:
        # æŒ‰å­—ç¬¦è¾“å‡ºï¼Œæ¨¡æ‹Ÿæµå¼
        words = list(part)
        chunk_size = 3  # æ¯æ¬¡è¾“å‡º3ä¸ªå­—ç¬¦
        for i in range(0, len(words), chunk_size):
            yield "".join(words[i:i+chunk_size])
            time.sleep(0.02)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# æš—çº¿ï¼šæˆ˜æœ¯æŠ¤ç›®é•œç”Ÿæˆï¼ˆLLM + å…³é”®è¯æ¨¡æ¿ + Mock æµå¼ï¼‰
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def generate_tactical_advice(
    query: str,
    retrieved_docs: List[Dict],
    api_key: str = None,
    model: str = "gpt-4o-mini"
) -> str:
    """ç”Ÿæˆå†…éƒ¨é”€å”®æˆ˜æœ¯æŒ‡å¯¼ï¼ˆMiller Heiman é£æ ¼ï¼‰"""
    # æ„å»ºæ–‡æ¡£æ‘˜è¦
    doc_names = []
    doc_summaries = []
    for doc in retrieved_docs[:3]:
        filename = doc.get('filename', 'æœªçŸ¥')
        content = doc.get('content', '')[:200]
        doc_summaries.append(f"- {filename}: {content}")
        doc_names.append(filename)
    context = "\n".join(doc_summaries) if doc_summaries else "ï¼ˆæ­¦å™¨åº“æ— ç›¸å…³æ–‡æ¡£ï¼‰"

    # æœ‰ API Key â†’ LLM ç”Ÿæˆ
    if api_key and api_key.strip() and len(api_key) > 10:
        try:
            client = openai.OpenAI(api_key=api_key)
            response = client.chat.completions.create(
                model=model,
                messages=[
                    {"role": "system", "content": TACTICAL_SYSTEM_PROMPT},
                    {"role": "user", "content": f"å®¢æˆ·é—®ï¼šã€Œ{query}ã€\n\næˆ‘æ–¹æ–‡æ¡£ï¼š\n{context}"}
                ],
                temperature=0.7,
                max_tokens=300
            )
            return response.choices[0].message.content
        except Exception:
            pass

    # æ—  API Key â†’ å…³é”®è¯æ™ºèƒ½æ¨¡æ¿
    return _keyword_tactical_fallback(query, doc_names)


def mock_stream_tactical_advice(query: str, retrieved_docs: List[Dict]):
    """æµå¼ Mock æˆ˜æœ¯æŠ¤ç›®é•œç”Ÿæˆå™¨"""
    advice = _keyword_tactical_fallback(
        query,
        [d.get('filename', 'æœªçŸ¥') for d in retrieved_docs[:3]]
    )
    # é€è¡Œæµå¼è¾“å‡º
    for line in advice.split("\n"):
        yield line + "\n"
        time.sleep(0.05)


def _keyword_tactical_fallback(query: str, doc_names: List[str]) -> str:
    """å…³é”®è¯é©±åŠ¨çš„æˆ˜æœ¯æ¨¡æ¿ï¼ˆæ—  API Key é™çº§æ–¹æ¡ˆï¼‰"""
    query_lower = query.lower()
    tactics = [
        (["ç›é›¾", "é˜²è…", "è€èš€", "è…èš€", "salt", "corrosion"],
         "é˜²è…æ€§èƒ½ä¸è€ä¹…æ€§æŒ‡æ ‡", "å®¢æˆ·å¯èƒ½åœ¨æ¨ªå‘å¯¹æ¯”ç«å“ç›é›¾æ•°æ®",
         "å¼ºè°ƒæˆ‘æ–¹ç›é›¾æµ‹è¯•æŠ¥å‘Šï¼ˆç¬¬ä¸‰æ–¹æƒå¨è®¤è¯ï¼‰ï¼Œå¼•å¯¼å±•ç¤ºå®é™…å·¥å†µæ¡ˆä¾‹",
         "é˜²è…æ˜¯æ ¸å¿ƒå–ç‚¹ï¼ŒæŠ¥ä»·å¯é€‚å½“åšæŒº"),
        (["ä»·æ ¼", "æŠ¥ä»·", "æˆæœ¬", "é¢„ç®—", "price", "cost", "tco"],
         "ä»·æ ¼æ•æ„Ÿåº¦é«˜ï¼Œè¿›å…¥æˆæœ¬åšå¼ˆ", "å®¢æˆ·å¯èƒ½å·²æœ‰ç«å“ä½ä»·æ–¹æ¡ˆ",
         "è½¬å‘TCOï¼ˆæ€»æ‹¥æœ‰æˆæœ¬ï¼‰åˆ†æï¼Œå¼ºè°ƒç»´æŠ¤å‘¨æœŸå’Œå¯¿å‘½ä¼˜åŠ¿",
         "å¯æä¾›é˜¶æ¢¯æŠ¥ä»·æˆ–å¢å€¼æœåŠ¡åŒ…"),
        (["æ¡ˆä¾‹", "é¡¹ç›®", "åº”ç”¨", "å·¥å‚", "ç‚¼åŒ–", "case", "reference", "success"],
         "å¸Œæœ›çœ‹å®é™…è½åœ°æ¡ˆä¾‹ä½œä¸ºå†³ç­–ä¾æ®", "ç¼ºä¹åŒè¡Œä¸šæ¡ˆä¾‹å¯èƒ½æˆä¸ºä¸¢å•å› ç´ ",
         "ç«‹å³å±•ç¤ºåŒè¡Œä¸šæ ‡æ†é¡¹ç›®ï¼Œçªå‡ºè¿è¡Œæ—¶é•¿å’Œé›¶æ•…éšœè®°å½•",
         "æ¡ˆä¾‹é©±åŠ¨å‹å®¢æˆ·å†³ç­–å¿«ï¼Œæ¨å¿«ç­¾çº¦èŠ‚å¥"),
        (["å‚æ•°", "è§„æ ¼", "åšåº¦", "é™„ç€åŠ›", "æ€§èƒ½", "spec", "performance"],
         "æ­£åœ¨æŠ€æœ¯é€‰å‹ï¼Œå…³æ³¨å…·ä½“æ€§èƒ½å‚æ•°", "æŠ€æœ¯ç»†èŠ‚ä¸å……åˆ†å¯èƒ½å¯¼è‡´é€‰å‹å‡ºå±€",
         "æä¾›å®Œæ•´è§„æ ¼ä¹¦ï¼Œä¸»åŠ¨å¯¹æ¯”è¡Œæ ‡ï¼Œå±•ç¤ºæŠ€æœ¯é¢†å…ˆç‚¹",
         "æŠ€æœ¯é©±åŠ¨å‹å®¢æˆ·ä»·æ ¼æ•æ„Ÿåº¦ä½"),
        (["äº¤æœŸ", "å·¥æœŸ", "æ–½å·¥", "delivery", "schedule"],
         "é¡¹ç›®æ—¶é—´èŠ‚ç‚¹å‹åŠ›å¤§", "äº¤æœŸä¸æ»¡è¶³å¯èƒ½ç›´æ¥å‡ºå±€",
         "å±•ç¤ºä¾›åº”é“¾å®åŠ›å’Œé¡¹ç›®ç®¡ç†èƒ½åŠ›ï¼Œæä¾›äº¤ä»˜è®¡åˆ’",
         "æ€¥å•å¯ä¸Šæµ®5-10%ï¼Œæä¾›åŠ æ€¥æœåŠ¡"),
        (["æ¸©å‡", "temperature", "heat", "temp", "å‘çƒ­"],
         "å…³æ³¨è®¾å¤‡æ•£çƒ­èƒ½åŠ›ä¸é•¿æœŸå¯é æ€§", "æ¸©å‡ä¸è¾¾æ ‡æ˜¯ç¡¬ä¼¤ï¼Œå®¢æˆ·å¯èƒ½æ‹¿æ­¤æ·˜æ±°ç«å“",
         "ä¸»åŠ¨å±•ç¤ºæ¸©å‡è¯•éªŒæŠ¥å‘Šï¼Œå¼ºè°ƒä¼˜äºGB/T 11022æ ‡å‡†é™å€¼",
         "æ¸©å‡åˆè§„æ˜¯å‡†å…¥é—¨æ§›ï¼ŒæŠ¥ä»·ä¸å¿…è®©æ­¥"),
        (["åˆ†æ–­", "çŸ­è·¯", "æ–­è·¯å™¨", "breaking", "short circuit", "ç‡ƒå¼§"],
         "è¯„ä¼°è®¾å¤‡æé™å·¥å†µä¸‹çš„å®‰å…¨æ€§", "åˆ†æ–­ä¸è¾¾æ ‡ç›´æ¥æ·˜æ±°ï¼Œæ— è°ˆåˆ¤ä½™åœ°",
         "å±•ç¤ºå‹å¼è¯•éªŒæŠ¥å‘Šï¼Œå¼ºè°ƒ31.5kA/80kAçš„æé™åˆ†æ–­æ•°æ®",
         "å®‰å…¨æ€§å–ç‚¹ï¼Œå®¢æˆ·æ„¿ä¸ºæ­¤æ”¯ä»˜æº¢ä»·"),
        (["æ™ºèƒ½", "ç‰©è”", "ç›‘æ§", "è¿œç¨‹", "iot", "smart", "æ— äººå€¼å®ˆ"],
         "å…³æ³¨æ™ºèƒ½åŒ–å‡çº§ä¸è¿ç»´é™æœ¬", "ç«å“å¯èƒ½å·²æœ‰ç‰©è”æ–¹æ¡ˆæŠ¥ä»·",
         "å±•ç¤ºæˆ‘æ–¹æ™ºèƒ½åŒ–ä¸“åˆ©å’Œè¿œç¨‹è¿ç»´å¹³å°demos",
         "æ™ºèƒ½æ¨¡å—æ¯›åˆ©é«˜ï¼Œå¯æ‰“åŒ…å®šä»·"),
    ]

    matched = None
    for keywords, focus, risk, talk, price in tactics:
        if any(kw in query_lower for kw in keywords):
            matched = (focus, risk, talk, price)
            break

    if not matched:
        doc_ref = f"å¼•å¯¼å®¢æˆ·æŸ¥çœ‹{'ã€'.join(doc_names)}ä¸­çš„å®æµ‹æ•°æ®" if doc_names else "å»ºè®®ä¸Šä¼ æ›´å¤šæŠ€æœ¯èµ„æ–™ä»¥å¢å¼ºå›ç­”ç²¾åº¦"
        matched = ("å®¢æˆ·å…³æ³¨çš„å…·ä½“æŠ€æœ¯ç»†èŠ‚", "éœ€ç¡®è®¤å®¢æˆ·æ˜¯å¦å¤„äºé€‰å‹æ¯”è¾ƒé˜¶æ®µ",
                   doc_ref, "åˆ©æ¶¦ç©ºé—´å……è¶³ï¼Œè‹¥å®¢æˆ·çŠ¹è±«å¯æ‰¿è¯ºå¢å€¼æœåŠ¡")

    return (
        f"ğŸ¯ **å®¢æˆ·å…³æ³¨ç‚¹ï¼š** {matched[0]}\n\n"
        f"âš ï¸ **æ½œåœ¨é£é™©ï¼š** {matched[1]}\n\n"
        f"ğŸ’¡ **è¯æœ¯å»ºè®®ï¼š** {matched[2]}\n\n"
        f"ğŸ’° **æŠ¥ä»·ç­–ç•¥ï¼š** {matched[3]}"
    )


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# å¼•ç”¨æ¥æºæ ¼å¼åŒ–
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def format_sources(sources: List[Dict]) -> str:
    """æ ¼å¼åŒ–å¼•ç”¨æ¥æº"""
    if not sources:
        return ""
    
    lines = ["### ğŸ“š å‚è€ƒæ¥æº\n"]
    for i, source in enumerate(sources, 1):
        similarity = source.get('similarity', source.get('score', 0))
        similarity_bar = "ğŸŸ¢" if similarity > 0.8 else "ğŸŸ¡" if similarity > 0.5 else "ğŸ”´"
        idx = source.get('index', i)
        filename = source.get('filename', 'æœªçŸ¥')
        asset_type = source.get('asset_type', source.get('metadata', {}).get('asset_type', 'æœªåˆ†ç±»'))
        lines.append(
            f"{similarity_bar} **[æ–‡æ¡£{idx}]** {filename} "
            f"({asset_type})"
        )
    
    return "\n".join(lines)


if __name__ == "__main__":
    print("RAG æ™ºèƒ½é—®ç­”æ¨¡å— V2 å·²åŠ è½½ â€” ç”µæ°”è¡Œä¸šä¸“ç”¨ç‰ˆ")
    
    test_docs = [
        {
            "filename": "æ¸©å‡è¯•éªŒæŠ¥å‘Š.pdf",
            "content": "å¼€å…³æŸœé¢å®šç”µæµ4000Aä¸‹æŒç»­è¿è¡Œ8å°æ—¶ï¼Œæ¯æ’æ¸©å‡62Kï¼ˆé™å€¼70Kï¼‰",
            "metadata": {"asset_type": "ğŸ›ï¸ [èƒŒä¹¦] æƒå¨èƒŒä¹¦", "source_type": "document"},
            "similarity": 0.92
        },
        {
            "filename": "ç‡ƒå¼§æµ‹è¯•.mp4",
            "content": "[è§†é¢‘è½¬å½•] 10kVçœŸç©ºæ–­è·¯å™¨å†…éƒ¨ç‡ƒå¼§è¯•éªŒï¼Œåˆ†æ–­ç”µæµ31.5kA",
            "metadata": {"asset_type": "ğŸ¯ [æ ‡å‡†] é€‰å‹æ’é›·", "source_type": "video"},
            "similarity": 0.85
        }
    ]
    
    # æµ‹è¯• context æ„å»º
    ctx = build_context_str(test_docs)
    print(f"\nä¸Šä¸‹æ–‡æ„å»º: {len(ctx)}å­—")
    
    # æµ‹è¯• mock æµå¼
    print("\n--- Mock æµå¼å®¢æˆ·å›å¤ ---")
    for chunk in mock_stream_client_response("æ¸©å‡æ€§èƒ½å¦‚ä½•ï¼Ÿ", test_docs):
        print(chunk, end="", flush=True)
    print("\n\n--- æˆ˜æœ¯æ¨¡æ¿ ---")
    print(generate_tactical_advice("æ¸©å‡æ€§èƒ½å¦‚ä½•ï¼Ÿ", test_docs))
