import base64

import openai
from openai import OpenAI

SYSTEM_PROMPT = (
    "ä½ æ˜¯ä¸€åèµ„æ·±å·¥ä¸šç”µæ°”é”€å”®ä¸“å®¶ã€‚è¯·å¯¹é”€å”®æ‹œè®¿å£è¿°è®°å½•è¿›è¡Œç»“æ„åŒ–æƒ…æŠ¥æå–ï¼Œ"
    "ä¸¥æ ¼è¿”å›ä»¥ä¸‹ JSON æ ¼å¼ï¼ˆ4+1 æƒ…æŠ¥æ¨¡å‹ï¼‰ï¼š\n"
    "{\n"
    "  \"current_status\": \"é¡¹ç›®ç°çŠ¶ã€é¢„ç®—ä¸è¿›åº¦ä¿¡æ¯\",\n"
    "  \"decision_chain\": [\n"
    "    {\"name\": \"å§“å\", \"title\": \"èŒåŠ¡\", \"phone\": \"è”ç³»æ–¹å¼(è‹¥æ— åˆ™è¿”å›null)\", "
    "\"attitude\": \"æ”¯æŒ/ä¸­ç«‹/åå¯¹\", \"soft_tags\": [\"æ ‡ç­¾1\", \"æ ‡ç­¾2\"]}\n"
    "  ],\n"
    "  \"competitor_info\": [\n"
    "    {\"name\": \"ç«å“åç§°\", \"quote\": \"æŠ¥ä»·(è‹¥æ— åˆ™è¿”å›null)\", "
    "\"strengths\": \"ä¼˜åŠ¿\", \"weaknesses\": \"åŠ£åŠ¿\", \"recent_actions\": \"è¿‘æœŸåŠ¨ä½œ\"}\n"
    "  ],\n"
    "  \"next_steps\": \"ä¸‹ä¸€æ­¥è¡ŒåŠ¨è®¡åˆ’æˆ–é”€å”®æ‰¿è¯º\",\n"
    "  \"gap_alerts\": [\"ç¼ºå£é¢„è­¦1\", \"ç¼ºå£é¢„è­¦2\"]\n"
    "}\n\n"
    "ä½œä¸ºä¸¥è‹›çš„é”€å”®æ€»ç›‘ï¼Œè¯·å®¡æŸ¥æ‹œè®¿è®°å½•å¹¶åœ¨ gap_alerts ä¸­æŒ‡å‡ºç¼ºå¤±çš„è‡´å‘½æƒ…æŠ¥ã€‚è§„åˆ™ï¼š\n"
    "1. æåˆ°äººç‰©ä½†æœªæä¾›ç”µè¯æˆ–è”ç³»æ–¹å¼ â†’ 'âš ï¸ æœªè·å– [å§“å] çš„è”ç³»æ–¹å¼'ã€‚\n"
    "2. æœªæåˆ°ä¸‹ä¸€æ­¥æ¨è¿›æ—¶é—´ â†’ 'âš ï¸ ç¼ºå°‘æ˜ç¡®çš„ä¸‹ä¸€æ­¥æ¨è¿›æ—¶é—´ç‚¹'ã€‚\n"
    "3. æœªç¡®è®¤é¡¹ç›®é¢„ç®— â†’ 'âš ï¸ æœªç¡®è®¤æœ€ç»ˆé¢„ç®—'ã€‚\n"
    "4. æœªè¯†åˆ«å…³é”®å†³ç­–äºº â†’ 'âš ï¸ æœªè¯†åˆ«å…³é”®å†³ç­–äºº'ã€‚\n"
    "å¦‚æœæƒ…æŠ¥å®Œç¾ï¼Œgap_alerts è¿”å›ç©ºæ•°ç»„ []ã€‚\n\n"
    "ä¸¥ç¦è¾“å‡ºä»»ä½• Markdown æ ‡è®°æˆ–å¤šä½™çš„è§£é‡Šè¯´æ˜ï¼Œåªè¿”å›åˆæ³•çš„ JSON å­—ç¬¦ä¸²ã€‚"
)


def encode_image(uploaded_file) -> str:
    """å°†ä¸Šä¼ çš„å›¾ç‰‡æ–‡ä»¶è½¬æ¢ä¸º Base64 å­—ç¬¦ä¸²ã€‚"""
    file_bytes = uploaded_file.read()
    uploaded_file.seek(0)  # é‡ç½®æŒ‡é’ˆä»¥ä¾¿å…¶ä»–åœ°æ–¹ç»§ç»­è¯»å–
    return base64.b64encode(file_bytes).decode("utf-8")


def parse_visit_log(api_key: str, raw_text: str) -> str:
    """è°ƒç”¨å¤§æ¨¡å‹ï¼Œå°†æ‹œè®¿æµæ°´è´¦æç‚¼ä¸ºç»“æ„åŒ– JSONï¼ˆ4+1 æƒ…æŠ¥æ¨¡å‹ï¼‰ã€‚"""
    client = OpenAI(api_key=api_key)

    response = client.chat.completions.create(
        model="gpt-3.5-turbo",
        messages=[
            {"role": "system", "content": SYSTEM_PROMPT},
            {"role": "user", "content": raw_text},
        ],
        temperature=0.2,
    )

    return response.choices[0].message.content


def parse_visit_log_with_image(api_key: str, raw_text: str, image_base64: str) -> str:
    """è°ƒç”¨å¤šæ¨¡æ€å¤§æ¨¡å‹ï¼ŒåŒæ—¶è§£ææ–‡å­—å£è¿° + å›¾ç‰‡æƒ…æŠ¥ï¼Œè¾“å‡º 4+1 JSONã€‚"""
    client = OpenAI(api_key=api_key)

    user_content = [
        {
            "type": "text",
            "text": (
                "ä»¥ä¸‹æ˜¯é”€å”®çš„æ–‡å­—å£è¿°è®°å½•ï¼Œè¯·ç»“åˆå£è¿°å†…å®¹å’Œå›¾ç‰‡ä¸­çš„ä¿¡æ¯"
                "ï¼ˆå¦‚åç‰‡ä¸Šçš„å§“å/ç”µè¯ã€è®¾å¤‡é“­ç‰Œå‚æ•°ã€æŠ¥ä»·å•ä»·æ ¼ç­‰ï¼‰ï¼Œ"
                "åˆå¹¶åä¸¥æ ¼æŒ‰ç…§ 4+1 JSON æ ¼å¼è¾“å‡ºã€‚\n\n"
                f"ã€é”€å”®å£è¿°è®°å½•ã€‘\n{raw_text}"
            ),
        },
        {
            "type": "image_url",
            "image_url": {
                "url": f"data:image/jpeg;base64,{image_base64}",
                "detail": "high",
            },
        },
    ]

    response = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[
            {"role": "system", "content": SYSTEM_PROMPT},
            {"role": "user", "content": user_content},
        ],
        temperature=0.2,
    )

    return response.choices[0].message.content


ADVISOR_PROMPT = (
    "ä½ æ˜¯ä¸€åç‹ è¾£çš„å·¥ä¸šé”€å”®å†›å¸ˆã€‚è¯·æ ¹æ®æä¾›çš„é¡¹ç›®å†å²è®°å½•ï¼Œå›ç­”ç”¨æˆ·é—®é¢˜ã€‚"
    "å¦‚æœæ˜¯å†™å‘¨æŠ¥ï¼Œè¯·ç”¨æå…¶ä¸“ä¸šçš„å•†åŠ¡é£æ ¼ï¼›å¦‚æœæ˜¯åˆ†æå±€åŠ¿ï¼Œè¯·ç›´è¨€ä¸è®³åœ°æŒ‡å‡ºé£é™©ã€‚"
    "å›ç­”å¿…é¡»åŸºäºå®¢è§‚æƒ…æŠ¥ï¼Œç»™å‡ºçŠ€åˆ©ã€ä¸“ä¸šçš„åˆ†æå’Œç­–ç•¥å»ºè®®ã€‚"
)


def chat_with_project(api_key: str, context_data: str, user_query: str) -> str:
    """åŸºäºé¡¹ç›®æƒ…æŠ¥ä¸Šä¸‹æ–‡ï¼Œä¸ AI å‚è°‹å¯¹è¯ï¼ˆéæµå¼ï¼‰ã€‚"""
    client = OpenAI(api_key=api_key)

    user_message = (
        f"ã€é¡¹ç›®å†å²æƒ…æŠ¥ä¸Šä¸‹æ–‡ã€‘\n{context_data}\n\n"
        f"ã€æˆ‘çš„é—®é¢˜ã€‘\n{user_query}"
    )

    response = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[
            {"role": "system", "content": ADVISOR_PROMPT},
            {"role": "user", "content": user_message},
        ],
        temperature=0.5,
    )

    return response.choices[0].message.content


def chat_with_project_stream(api_key: str, context_data: str, messages: list):
    """åŸºäºé¡¹ç›®æƒ…æŠ¥ä¸Šä¸‹æ–‡ï¼Œä¸ AI å‚è°‹å¯¹è¯ï¼ˆæµå¼è¾“å‡ºï¼‰ã€‚
    
    messages: å®Œæ•´çš„å¯¹è¯å†å² [{"role": "user"/"assistant", "content": "..."}]
    è¿”å›ä¸€ä¸ªç”Ÿæˆå™¨ï¼Œé€ chunk yield æ–‡æœ¬ã€‚
    """
    client = OpenAI(api_key=api_key)

    # å°†é¡¹ç›®æƒ…æŠ¥æ³¨å…¥ system prompt
    system_msg = (
        f"{ADVISOR_PROMPT}\n\n"
        f"ã€å½“å‰é¡¹ç›®çš„å…¨éƒ¨å†å²æƒ…æŠ¥æ•°æ®ã€‘\n{context_data}"
    )

    api_messages = [{"role": "system", "content": system_msg}] + messages

    stream = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=api_messages,
        temperature=0.5,
        stream=True,
    )

    for chunk in stream:
        delta = chunk.choices[0].delta
        if delta.content:
            yield delta.content


def generate_quiz(api_key: str, context_data: str, blind_spots: str = "æ— ") -> str:
    """åŸºäºé¡¹ç›®æƒ…æŠ¥ + å†å²ç›²ç‚¹ï¼Œç”Ÿæˆä¸€é“ä¸‰ç»´å®æˆ˜æƒ…æ™¯æ¨¡æ‹Ÿæµ‹éªŒé¢˜ã€‚"""
    client = OpenAI(api_key=api_key)

    coach_prompt = (
        "ä½ æ˜¯ä¸€åé¡¶çº§çš„å·¥ä¸šç”µæ°”é”€å”®æ•™ç»ƒå…¼æŠ€æœ¯æ€»å·¥ã€‚\n"
        "è¯·ä»”ç»†é˜…è¯»è¯¥é”€å”®çš„é¡¹ç›®æ±‡æŠ¥å†…å®¹ï¼Œä»¥åŠä»–è¿‡å»çš„"
        f"ã€çŸ¥è¯†ç›²ç‚¹ï¼š{blind_spots}ã€‘ã€‚\n"
        "ä½ éœ€è¦ä»ä»¥ä¸‹ä¸‰ä¸ªç»´åº¦ä¸­ï¼Œé’ˆå¯¹ä»–çš„ç›²ç‚¹æˆ–å½“å‰é¡¹ç›®çš„è–„å¼±ç¯èŠ‚ï¼Œ"
        "æå‡ºä¸€é“æå…¶åˆé’»çš„å®æˆ˜æµ‹éªŒé¢˜ï¼š\n\n"
        "1. å•†åŠ¡åšå¼ˆï¼ˆå¦‚ï¼šåº”å¯¹ç«å“ä½ä»·ã€æå®šå…³é”®å†³ç­–äººã€æ¨è¿›é‡‡è´­æµç¨‹ï¼‰ã€‚\n"
        "2. æŠ€æœ¯æ–¹æ¡ˆï¼ˆå¦‚ï¼šé«˜ä½å‹æŸœæ ¸å¿ƒå‚æ•°è§£æã€æˆ‘æ–¹æ–¹æ¡ˆçš„ç”µæ°”æŠ€æœ¯ä¼˜åŠ¿ã€"
        "è§£ç­”å®¢æˆ·å¯¹å…ç»´æŠ¤è®¾è®¡çš„æŠ€æœ¯è´¨ç–‘ï¼‰ã€‚\n"
        "3. è¡Œä¸šè®¤çŸ¥ï¼ˆå¦‚ï¼šè¯¥å®¢æˆ·æ‰€åœ¨è¡Œä¸šçš„æœ€æ–°æ”¿ç­–å¯¼å‘ã€å…¸å‹ç”¨ç”µè´Ÿè·ç‰¹å¾ã€"
        "ä¸Šä¸‹æ¸¸ç—›ç‚¹ï¼‰ã€‚\n\n"
        "è¦æ±‚ï¼š\n"
        "- ä¸è¦è¯´åºŸè¯ï¼Œç»“åˆé¡¹ç›®å½“å‰çœŸå®æƒ…å†µç›´æ¥æé—®ã€‚\n"
        "- é—®é¢˜å¿…é¡»å…·æœ‰æå¼ºçš„å‹è¿«æ„Ÿå’Œå®æˆ˜æ„ä¹‰ã€‚\n"
        "- å­—æ•°ä¸¥æ ¼æ§åˆ¶åœ¨ 100 å­—ä»¥å†…ã€‚"
    )

    response = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[
            {"role": "system", "content": coach_prompt},
            {"role": "user", "content": f"ã€é¡¹ç›®å†å²æƒ…æŠ¥ã€‘\n{context_data}"},
        ],
        temperature=0.8,
    )

    return response.choices[0].message.content


CRITIQUE_PROMPT = (
    "ä½ æ˜¯ä¸€åæå…¶ä¸¥è‹›ã€ç‹ è¾£çš„å·¥ä¸šé”€å”®æ€»ç›‘ã€‚è¯·è¯„ä¼°é”€å”®äººå‘˜å¯¹å®æˆ˜é—®é¢˜çš„å›ç­”ã€‚\n"
    "å¦‚æœå›ç­”æ˜¯\"åšå¥½å®¢æƒ…\"ã€\"åŠ å¼ºæ²Ÿé€š\"ç­‰å‡å¤§ç©ºçš„åºŸè¯ï¼Œè¯·ç»™äºˆæä½çš„åˆ†æ•°å¹¶ä¸¥å‰æ‰¹è¯„ã€‚\n"
    "ä½ å¿…é¡»ä¸¥æ ¼è¿”å›ä»¥ä¸‹ JSON æ ¼å¼ï¼š\n"
    "{\n"
    "  \"score\": æ•°å­— (0-100çš„è¯„åˆ†),\n"
    "  \"critique\": \"ä¸¥å‰ä¸”ç›´æŒ‡æ ¸å¿ƒçš„ç‚¹è¯„è¯ï¼Œä¸è¶…è¿‡150å­—\",\n"
    "  \"blind_spots\": [\"ç›²ç‚¹æ ‡ç­¾1\", \"ç›²ç‚¹æ ‡ç­¾2\"] "
    "(æå–1-3ä¸ªä»–ç¼ºä¹çš„è®¤çŸ¥ç›²ç‚¹ï¼Œå¦‚\"ç¼ºä¹å…·ä½“æŠ€æœ¯åé©³è¯æœ¯\"ã€\"å¯¹ç«å“äº¤æœŸæ— é¢„æ¡ˆ\")\n"
    "}\n\n"
    "ä¸¥ç¦è¾“å‡ºä»»ä½• Markdown æ ‡è®°æˆ–å¤šä½™çš„è§£é‡Šè¯´æ˜ï¼Œåªè¿”å›åˆæ³•çš„ JSON å­—ç¬¦ä¸²ã€‚"
)


def critique_answer(api_key: str, quiz: str, user_answer: str) -> str:
    """è¯„ä¼°é”€å”®äººå‘˜çš„åº”å¯¹ç­–ç•¥ï¼Œè¿”å› JSON æ ¼å¼çš„è¯„åˆ†ã€ç‚¹è¯„å’Œç›²ç‚¹ã€‚"""
    client = OpenAI(api_key=api_key)

    user_message = (
        f"ã€æµ‹éªŒé¢˜ç›®ã€‘\n{quiz}\n\n"
        f"ã€é”€å”®äººå‘˜çš„å›ç­”ã€‘\n{user_answer}"
    )

    response = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[
            {"role": "system", "content": CRITIQUE_PROMPT},
            {"role": "user", "content": user_message},
        ],
        temperature=0.3,
    )

    return response.choices[0].message.content


TEAM_REPORT_PROMPT = (
    "ä½ æ˜¯ä¸€åé¡¶çº§çš„å·¥ä¸šç”µæ°”é”€å”®èµ‹èƒ½ä¸“å®¶ã€‚\n"
    "è¯·æ ¹æ®ä»¥ä¸‹æ±‡æ€»çš„ã€å›¢é˜Ÿè¿‘æœŸæš´éœ²çš„çŸ¥è¯†ç›²ç‚¹ã€‘ï¼Œè¿›è¡Œæ·±åº¦å½’çº³åˆ†æã€‚\n"
    "è¯·è¾“å‡ºä¸€ä»½ç®€æ˜æ‰¼è¦çš„ä½“æ£€æŠ¥å‘Šï¼Œå¿…é¡»åŒ…å«ï¼š\n"
    "1. æ ¸å¿ƒå…±æ€§çŸ­æ¿ï¼ˆå¤§å®¶æ™®éæ¬ ç¼ºçš„3ä¸ªèƒ½åŠ›ï¼‰ã€‚\n"
    "2. ä¸‹ä¸€æ­¥é’ˆå¯¹æ€§åŸ¹è®­å»ºè®®ï¼ˆå…·ä½“åˆ°åº”è¯¥è¡¥é½ä»€ä¹ˆè¯æœ¯æˆ–æŠ€æœ¯çŸ¥è¯†ï¼‰ã€‚\n"
    "ä¸è¦åºŸè¯ï¼Œç›´æ¥è¾“å‡ºä¸“ä¸šåˆ†æã€‚"
)


def generate_team_report(api_key: str, blind_spots_summary: str) -> str:
    """åŸºäºå›¢é˜Ÿç›²ç‚¹æ±‡æ€»æ•°æ®ï¼Œç”Ÿæˆå›¢é˜Ÿèƒ½åŠ›ä½“æ£€æŠ¥å‘Šã€‚"""
    client = OpenAI(api_key=api_key)

    response = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[
            {"role": "system", "content": TEAM_REPORT_PROMPT},
            {"role": "user", "content": f"ã€å›¢é˜Ÿè¿‘æœŸæš´éœ²çš„çŸ¥è¯†ç›²ç‚¹æ±‡æ€»ã€‘\n{blind_spots_summary}"},
        ],
        temperature=0.5,
    )

    return response.choices[0].message.content


FOLLOWUP_PROMPTS_EMAIL = (
    'ä½ æ˜¯ä¸€åæå…¶ä¸“ä¸šçš„å·¥ä¸šå¤§å®¢æˆ·é”€å”®æ€»ç›‘ã€‚\n'
    'è¯·æ ¹æ®æä¾›çš„ã€é¡¹ç›®å†å²æƒ…æŠ¥ã€‘ï¼Œä¸ºå‰çº¿é”€å”®å†™ä¸€å°å‘ç»™è¯¥é¡¹ç›®æ ¸å¿ƒå†³ç­–äººçš„è·Ÿè¿›é‚®ä»¶ã€‚\n'
    'è¦æ±‚ï¼š\n'
    '1. å¿…é¡»åŒ…å«é‚®ä»¶æ ‡é¢˜è¡Œï¼ˆSubject:ï¼‰ã€‚\n'
    '2. ç§°å‘¼å¿…é¡»å‡†ç¡®ï¼ˆä»æƒ…æŠ¥ä¸­æå–æœ€é«˜çº§åˆ«çš„å†³ç­–äººï¼‰ã€‚\n'
    '3. é‚®ä»¶æ­£æ–‡ç›´å‡»å®¢æˆ·ç—›ç‚¹ï¼Œå·§å¦™æš—ç¤ºæˆ‘ä»¬åœ¨æŠ€æœ¯æˆ–äº¤æœŸä¸Šä¼˜äºç«äº‰å¯¹æ‰‹ï¼Œ'
    'ä½†ä¸å¯å¸¦æœ‰æ˜æ˜¾æ”»å‡»æ€§ã€‚\n'
    '4. å¿…é¡»åŒ…å«ä¸€ä¸ªæ˜ç¡®çš„ä¸‹ä¸€æ­¥æ¨è¿›åŠ¨ä½œï¼ˆä¾‹å¦‚è¯·æ±‚ä¸‹å‘¨ä¸‰è¿›è¡ŒæŠ€æœ¯æ±‡æŠ¥ï¼‰ã€‚\n'
    '5. ç»“å°¾å¿…é¡»æœ‰ä¸“ä¸šè½æ¬¾ã€‚\n'
    '6. è¯­æ°”ä¸“ä¸šã€è¯šæ³ã€ä¸å‘ä¸äº¢ã€‚'
)


def _build_wechat_followup_prompt(target_person: str) -> str:
    return (
        f'ä½ æ˜¯ä¸€åæ·±è°™äººæƒ…ä¸–æ•…çš„é¡¶çº§å·¥ä¸šé”€å”®ã€‚\n'
        f'è¯·ä¸ºå½“å‰é¡¹ç›®å‘ã€{target_person}ã€‘å†™ä¸€æ¡å¾®ä¿¡è·Ÿè¿›æ¶ˆæ¯ã€‚\n\n'
        f'è¯·é¦–å…ˆä»”ç»†é˜…è¯»ä»¥ä¸‹ã€é¡¹ç›®å†å²æƒ…æŠ¥ã€‘ï¼Œé‡ç‚¹åˆ†æã€{target_person}ã€‘çš„'
        f'èŒåŠ¡ã€ç—›ç‚¹ã€æ€§æ ¼æ ‡ç­¾ï¼Œä»¥åŠæˆ‘æ–¹ç›®å‰ä¸ä»–çš„ã€å…³ç³»æ·±åº¦ã€‘ã€‚\n\n'
        f'ã€å¼ºåˆ¶æ€§å®šåˆ¶åŒ–åŸåˆ™ã€‘ï¼š\n'
        f'1. çœ‹äººä¸‹èœç¢Ÿï¼ˆåŸºäºè§’è‰²ï¼‰ï¼š\n'
        f'   - å¦‚æœæ˜¯æŠ€æœ¯/æ€»å·¥ï¼Œå¤šæå…ç»´æŠ¤ã€ç¨³å®šæ€§ã€æŠ€æœ¯éªŒè¯ï¼›\n'
        f'   - å¦‚æœæ˜¯é‡‡è´­/å•†åŠ¡ï¼Œå¤šæäº¤æœŸä¿éšœã€ç»¼åˆæˆæœ¬ä¼˜åŠ¿ï¼›\n'
        f'   - å¦‚æœæ˜¯å¤§è€æ¿/å†³ç­–äººï¼Œå¤šææ— å¿§è¿è¡Œã€å¤§å±€è§‚ã€é•¿æœŸåˆä½œä»·å€¼ã€‚\n'
        f'2. æŠŠæ¡åˆ†å¯¸æ„Ÿï¼ˆåŸºäºå…³ç³»æ·±åº¦ï¼‰ï¼š\n'
        f'   - å¦‚æœæƒ…æŠ¥æ˜¾ç¤ºè¿˜æ²¡åŠ ä¸Šå¾®ä¿¡æˆ–åˆšæ¥è§¦ï¼Œè¯­æ°”å¿…é¡»ä¸“ä¸šã€è°¦é€Šã€ç›´æ¥è¡¨æ˜ä»·å€¼ï¼›\n'
        f'   - å¦‚æœæƒ…æŠ¥æ˜¾ç¤ºå·²ç»æ˜¯ç†Ÿäºº/å†…çº¿ï¼ˆå¦‚ç»å¸¸æ‹œè®¿ã€é€éœ²äº†ç«å“åº•ä»·ï¼‰ï¼Œ'
        f'è¯­æ°”è¦æå…¶è‡ªç„¶ã€å£è¯­åŒ–ï¼Œç”šè‡³å¯ä»¥å¸¦ç‚¹è°ƒä¾ƒã€‚\n'
        f'3. åƒçœŸäººä¸€æ ·è‡ªç„¶ï¼Œç¦æ­¢ç¿»è¯‘è…”ã€‚\n'
        f'4. å¼€å¤´ç›´æ¥ç‚¹æ˜äº‹ç”±ï¼Œä¸­é—´è½¯æ¤å…¥æˆ‘æ–¹ä¼˜åŠ¿ï¼Œç»“å°¾ç•™ä¸€ä¸ªè½»æ¾çš„äº’åŠ¨é’©å­ã€‚\n'
        f'5. å­—æ•°æ§åˆ¶åœ¨ 150 å­—ä»¥å†…ï¼Œé€‚å½“ä½¿ç”¨ emojiã€‚'
    )


def generate_followup_email(api_key: str, context_data: str,
                            channel: str = "email",
                            target_person: str = "å…³é”®å†³ç­–äºº",
                            project_stage: str = "åˆæœŸæ¥è§¦",
                            use_top_to_top: bool = False,
                            shared_history: str = "",
                            is_director: bool = False,
                            subordinate_name: str = "") -> str:
    client = OpenAI(api_key=api_key)

    if channel == "wechat":
        prompt = _build_wechat_followup_prompt(target_person)
    else:
        prompt = FOLLOWUP_PROMPTS_EMAIL

    # é˜¶æ®µæ„ŸçŸ¥æ³¨å…¥
    prompt += (
        f"\nã€å½“å‰é¡¹ç›®é˜¶æ®µã€‘ï¼š{project_stage}ã€‚"
        f"è¯·åŠ¡å¿…æ ¹æ®è¯¥é˜¶æ®µçš„ç‰¹å¾è°ƒæ•´è¯æœ¯ã€‚"
        f"ä¾‹å¦‚ï¼šåˆæœŸé‡åœ¨å±•ç¤ºä¸“ä¸šä¸ç ´å†°ï¼›æŠ¥ä»·æœŸé‡åœ¨ä¼ é€’ä»·å€¼ï¼›"
        f"å•†åŠ¡åƒµæŒæœŸé‡åœ¨æ‰“ç ´ä¿¡æ¯å·®æˆ–æ–½å‹ã€‚"
    )

    # é«˜ç®¡ååŒæ³¨å…¥
    if use_top_to_top:
        prompt += (
            "\nã€é«˜ç®¡ååŒæˆ˜ç•¥ã€‘ï¼šå¯¹æ–¹æ˜¯å…¬å¸é•¿çº¿å®¢æˆ·ï¼Œ"
            "è¯·åœ¨è¯æœ¯ä¸­æå…¶å·§å¦™ã€è‡ªç„¶åœ°å¼•å…¥æˆ‘æ–¹é«˜ç®¡/è€é¢†å¯¼ã€‚"
            "ä¸è¦ç”Ÿç¡¬ï¼Œè¦ä½“ç°å‡ºæˆ‘æ–¹é«˜ç®¡å¯¹è¯¥é¡¹ç›®çš„æåº¦é‡è§†ï¼Œ"
            "æˆ–è€…æš—ç¤ºæˆ‘æ–¹é«˜ç®¡ä¸å¯¹æ–¹æœ‰å†å²åˆä½œæ¸Šæº/æ„Ÿæƒ…åŸºç¡€ã€‚"
            "ç›®çš„æ˜¯é€šè¿‡å€ŸåŠ›æ‰“åŠ›ï¼Œä¿ƒæˆä¸‹ä¸€æ­¥çš„é«˜å±‚ä¼šé¢æˆ–è·¨è¿‡åŸºå±‚é˜»ç¢ã€‚"
        )
        if shared_history.strip():
            prompt += (
                f"\nã€å†å²æ¸Šæºç´ æã€‘ï¼š{shared_history.strip()}ã€‚"
                f"è¯·å°†è¿™æ®µå…±åŒç»å†è‡ªç„¶åœ°ç»‡å…¥è¯æœ¯ä¸­ï¼Œ"
                f"å”¤é†’å¯¹æ–¹çš„æƒ…ç»ªè®°å¿†ï¼Œæ‹‰è¿‘è·ç¦»ï¼Œä½†ä¸è¦æ˜¾å¾—åˆ»æ„ç…½æƒ…ã€‚"
            )

    # æ€»ç›‘åŠ©é”€æ¨¡å¼æ³¨å…¥
    if is_director:
        sub = subordinate_name.strip() or "æˆ‘ä»¬çš„é¡¹ç›®è´Ÿè´£äºº"
        prompt += (
            f"\nã€æ€»ç›‘åŠ©é”€æ¨¡å¼ã€‘ï¼šå½“å‰å‘é€è€…èº«ä»½æ˜¯é”€å”®æ€»ç›‘/é«˜ç®¡ï¼Œä¸æ˜¯æ™®é€šé”€å”®ã€‚"
            f"è¯·ä»¥é«˜ç®¡è§†è§’æ’°å†™è¯æœ¯ï¼Œä½“ç°é«˜å±‚äº²è‡ªå…³æ³¨çš„åˆ†é‡æ„Ÿã€‚"
            f"è¯æœ¯ä¸­éœ€è‡ªç„¶åœ°æåŠä¸‹å±ã€{sub}ã€‘ï¼Œ"
            f"ä¾‹å¦‚ï¼š'{sub}è·Ÿæˆ‘æ±‡æŠ¥äº†è´µå¸é¡¹ç›®çš„è¿›å±•ï¼Œæˆ‘éå¸¸é‡è§†...' "
            f"æˆ– 'æˆ‘ç‰¹æ„è®©{sub}æŠŠæœ€æ–°æ–¹æ¡ˆç»™æ‚¨åŒæ­¥ä¸€ä¸‹...' "
            f"ç›®çš„æ˜¯é€šè¿‡é«˜ç®¡èº«ä»½è¿›è¡Œé™ç»´æ‰“å‡»ï¼ŒåŒæ—¶æŠ¬é«˜ä¸‹å±åœ¨å®¢æˆ·å¿ƒä¸­çš„åœ°ä½ã€‚"
        )

    response = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[
            {"role": "system", "content": prompt},
            {"role": "user", "content": f"ã€é¡¹ç›®å†å²æƒ…æŠ¥ã€‘\n{context_data}"},
        ],
        temperature=0.7 if channel == "wechat" else 0.6,
    )

    return response.choices[0].message.content


TECH_PROMPTS = {
    "wechat": (
        'ä½ æ˜¯ä¸€åèµ„æ·±çš„å·¥ä¸šç”µæ°”æŠ€æœ¯æ€»å·¥ã€‚\n'
        'è¯·æ ¹æ®ã€é¡¹ç›®å†å²æƒ…æŠ¥ã€‘ï¼Œç”Ÿæˆä¸€æ®µé€‚åˆåœ¨æ‰‹æœºä¸Šé˜…è¯»çš„æŠ€æœ¯è¦ç‚¹åˆ—è¡¨ã€‚\n'
        'è¦æ±‚ï¼š\n'
        '1. æ¯è¡Œä¸è¶…è¿‡ 15 å­—ï¼Œç”¨ âœ… ç­‰ç¬¦å·åˆ†ç‚¹ã€‚\n'
        '2. çªå‡ºæˆ‘æ–¹æ–¹æ¡ˆçš„æ ¸å¿ƒæŠ€æœ¯äº®ç‚¹å’Œå…ç»´æŠ¤ä¼˜åŠ¿ã€‚\n'
        '3. å¯¹äºå…·ä½“çš„è®¾å¤‡å‚æ•°ï¼ˆå¦‚å°ºå¯¸ã€ç”µæµï¼‰ï¼Œå¦‚æœä½ åœ¨å†å²æƒ…æŠ¥ä¸­æ²¡çœ‹åˆ°ï¼Œ'
        'è¯·ä½¿ç”¨ [éœ€å¡«å…¥æˆ‘æ–¹å…·ä½“å‚æ•°] å ä½ï¼Œä¸è¦çç¼–ã€‚'
    ),
    "email": (
        'ä½ æ˜¯ä¸€åèµ„æ·±çš„å·¥ä¸šç”µæ°”æŠ€æœ¯æ€»å·¥ã€‚\n'
        'è¯·æ ¹æ®æä¾›çš„ã€é¡¹ç›®å†å²æƒ…æŠ¥ã€‘ï¼Œç”Ÿæˆä¸€ä»½ç²¾ç‚¼çš„æŠ€æœ¯æ–¹æ¡ˆæ‘˜è¦ï¼Œ'
        'ç”¨äºæäº¤ç»™å®¢æˆ·çš„æŠ€æœ¯è¯„å®¡å›¢ã€‚\n'
        'è¦æ±‚ï¼š\n'
        '1. ç½—åˆ—æˆ‘æ–¹è®¾å¤‡çš„ç¡¬æ ¸æŠ€æœ¯å‚æ•°ä¸å…ç»´æŠ¤è®¾è®¡ä¼˜åŠ¿ã€‚\n'
        '2. é’ˆå¯¹å®¢æˆ·é¡¹ç›®éœ€æ±‚é€æ¡åŒ¹é…æˆ‘æ–¹æ–¹æ¡ˆçš„æŠ€æœ¯äº®ç‚¹ã€‚\n'
        '3. å¦‚æœ‰ç«å“æƒ…æŠ¥ï¼Œä»¥å¯¹æ¯”è¡¨æ ¼å½¢å¼çªå‡ºå·®å¼‚åŒ–ä¼˜åŠ¿ã€‚\n'
        '4. å¯¹äºå…·ä½“çš„è®¾å¤‡å‚æ•°ï¼ˆå¦‚å°ºå¯¸ã€ç”µæµï¼‰ï¼Œå¦‚æœä½ åœ¨å†å²æƒ…æŠ¥ä¸­æ²¡çœ‹åˆ°ï¼Œ'
        'è¯·ä½¿ç”¨ [éœ€å¡«å…¥æˆ‘æ–¹å…·ä½“å‚æ•°] å ä½ï¼Œä¸è¦çç¼–ã€‚\n'
        '5. è¯­è¨€ç®€æ´ä¸“ä¸šï¼Œé€‚åˆå·¥ç¨‹å¸ˆé˜…è¯»ã€‚'
    ),
}


def generate_tech_summary(api_key: str, context_data: str,
                          channel: str = "email",
                          tech_competitor: str = "",
                          tech_status: str = "",
                          tech_pain_points: list = None,
                          tech_role: list = None) -> str:
    """åŸºäºé¡¹ç›®æƒ…æŠ¥ + å››ç»´é…ç½®ï¼Œç”Ÿæˆ Miller Heiman ä½“ç³»çš„æŠ€æœ¯ä¸å•†åŠ¡èåˆæ–¹æ¡ˆæ‘˜è¦ã€‚"""
    client = OpenAI(api_key=api_key)

    pain_points_str = "ã€".join(tech_pain_points) if tech_pain_points else "æœªæ˜ç¡®å…·ä½“ç—›ç‚¹"
    role_str = "ã€".join(tech_role) if tech_role else "æœªæŒ‡å®š"

    prompt = (
        f"ä½ æ˜¯ä¸€åæ·±è°™å¤æ‚å¤§å®¢æˆ·é”€å”®ï¼ˆMiller Heimanä½“ç³»ï¼‰çš„é¡¶çº§å·¥ä¸šç”µæ°”æ€»å·¥ç¨‹å¸ˆã€‚\n"
        f"è¯·ä¸ºå½“å‰é¡¹ç›®çš„ç‰¹å®šå—ä¼—ï¼Œç”Ÿæˆä¸€ä»½æå…·æ€ä¼¤åŠ›çš„ã€æŠ€æœ¯ä¸å•†åŠ¡èåˆæ–¹æ¡ˆæ‘˜è¦ã€‘ã€‚\n\n"

        f"ã€ğŸ¯ å››ç»´åˆ¶å¯¼å‚æ•°ã€‘ï¼š\n"
        f"1. æ²Ÿé€šå¯¹è±¡èº«å…¼çš„è§’è‰²ï¼š{role_str}\n"
        f"2. æ˜ç¡®çš„å¯¹æ¯”å‹å•†ï¼š{tech_competitor if tech_competitor.strip() else 'æœªæŒ‡å®š'}"
        f" (å¦‚æœæŒ‡å®šäº†å‹å•†ï¼Œè¯·é‡‡å–ä¸“ä¸šä¸”éšè”½çš„æ‹‰è¸©ç­–ç•¥ï¼Œå¼ºè°ƒæˆ‘ä»¬çš„å·®å¼‚åŒ–ä¼˜åŠ¿)\n"
        f"3. å®¢æˆ·å½“å‰ç³»ç»Ÿç°çŠ¶ï¼š{tech_status if tech_status.strip() else 'æœªæä¾›'}\n"
        f"4. å®¢æˆ·æ ¸å¿ƒç—›ç‚¹ï¼š{pain_points_str}\n\n"

        f"ã€è¾“å‡ºç»å¯¹çº¢çº¿ã€‘ï¼š\n"
        f"1. å¿…é¡»å®Œå…¨è´´åˆã€{role_str}ã€‘çš„å¤åˆè§†è§’ï¼"
        f"å¦‚æœä»–æ—¢æ˜¯å†³ç­–è€…åˆæ˜¯å½±å“è€…ï¼Œä½ æ—¢è¦è®²ROIä¹Ÿè¦è®²æŠ€æœ¯åˆè§„ï¼›"
        f"å¦‚æœæ˜¯ç»™ã€Œæ•™ç»ƒ/å†…çº¿ã€ï¼Œè¦æä¾›èƒ½ç›´æ¥å¤åˆ¶ç”¨äºå‘ä¸Šçº§æ±‡æŠ¥çš„æ§æ ‡ç†ç”±ã€‚\n"
        f"2. é’ˆå¯¹ã€æ ¸å¿ƒç—›ç‚¹ã€‘ï¼Œå¿…é¡»æå‡ºæˆ‘ä»¬æ–¹æ¡ˆä¸­å¯¹åº”çš„"
        f"ã€Œç‰¹å¾(Feature) + ä¼˜åŠ¿(Advantage) + åˆ©ç›Š(Benefit)ã€ï¼\n"
        f"3. è¾“å‡º 3-4 ä¸ªæ ¸å¿ƒæ®µè½ï¼Œæ‹’ç»ç½—åˆ—çŸ­å¥ï¼Œæ‹’ç»å‡å¤§ç©ºçš„åºŸè¯ã€‚"
        f"å¿…é¡»åƒä¸€ä»½ä¸“ä¸šçš„ã€å¯ç›´æ¥å¾®ä¿¡è½¬å‘ç»™è¯¥è§’è‰²çš„æ­£å¼æ±‡æŠ¥æ‘˜è¦ã€‚\n"
        f"4. é‡åˆ°ç¼ºå¤±çš„å…·ä½“è®¾å¤‡å‚æ•°ï¼Œä¸¥æ ¼ä½¿ç”¨ [éœ€å¡«å…¥å…·ä½“å‚æ•°] å ä½ã€‚"
    )

    # å¾®ä¿¡æ¸ é“è¿½åŠ ç§»åŠ¨ç«¯æ ¼å¼çº¦æŸ
    if channel == "wechat":
        prompt += (
            "\n5. å› ä¸ºæœ€ç»ˆé€šè¿‡å¾®ä¿¡å‘é€ï¼Œæ¯ä¸ªæ®µè½æ§åˆ¶åœ¨ 3-5 è¡Œä¹‹å†…ï¼Œ"
            "é€‚å½“ä½¿ç”¨ âœ… ğŸ“Š ğŸ”§ ç­‰ç¬¦å·å¢å¼ºå¯è¯»æ€§ã€‚"
        )

    response = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[
            {"role": "system", "content": prompt},
            {"role": "user", "content": f"ã€é¡¹ç›®å†å²æƒ…æŠ¥ã€‘\n{context_data}"},
        ],
        temperature=0.5,
    )

    return response.choices[0].message.content


def generate_insider_ammo(api_key: str, context_data: str,
                          channel: str = "wechat",
                          target_person: str = "æ•™ç»ƒ/å†…çº¿",
                          project_stage: str = "åˆæœŸæ¥è§¦",
                          leader_attitude: str = "",
                          leader_history: str = "") -> str:
    """ä¸ºæ•™ç»ƒ/å†…çº¿ä¸€æ¬¡æ€§ç”Ÿæˆ 3 ç§ä¾§é‡ç‚¹çš„å‘ä¸Šç®¡ç†è¯æœ¯ã€‚"""
    client = OpenAI(api_key=api_key)

    prompt = (
        f"ä½ æ˜¯ä¸€åæ·±è°™å¤æ‚é”€å”®åšå¼ˆå’ŒèŒåœºå‘ä¸Šç®¡ç†çš„é¡¶çº§å†›å¸ˆã€‚\n"
        f"è¯·æ ¹æ®ä»¥ä¸‹é¡¹ç›®æƒ…æŠ¥ï¼Œä¸ºæˆ‘ä»¬çš„ã€Œå†…çº¿ï¼ˆæ•™ç»ƒï¼‰ã€å†™å‡º 3 ä¸ªä¸åŒä¾§é‡ç‚¹çš„ã€å†…éƒ¨æ±‡æŠ¥è¯æœ¯ã€‘ã€‚\n\n"

        f"ã€ğŸ¯ ç»ˆæé¶å‘ï¼šé¢†å¯¼å¿ƒç†ç”»åƒã€‘ï¼š\n"
        f"ä½ è¦æ±‡æŠ¥çš„è¿™ä½æœ€é«˜å†³ç­–è€…ï¼Œç›®å‰çš„å¿ƒç†çŠ¶æ€æ˜¯ï¼šã€{leader_attitude if leader_attitude else 'æœªæŒ‡å®š'}ã€‘ã€‚\n"
        f"ä»–ç›®å‰çš„èƒŒæ™¯è½¨è¿¹æ˜¯ï¼šã€{leader_history if leader_history else 'æœªæŒ‡å®š'}ã€‘ã€‚\n"
        f"ä½ çš„è¯æœ¯å¿…é¡»**æå…¶ç²¾å‡†åœ°åˆ‡ä¸­/åˆ©ç”¨ä»–ç°åœ¨çš„è¿™ä¸ªå¿ƒç†è½¯è‚‹æˆ–ä¿¡ä»»åŸºç¡€**ï¼\n\n"

        f"ã€æ ¸å¿ƒåŸåˆ™ã€‘ï¼š\n"
        f"1. ç»å¯¹ä¼ªè£…ï¼šè¯­æ°”å¿…é¡»å®Œå…¨æ˜¯ã€Œå®¢æˆ·å†…éƒ¨æŠ€æœ¯è´Ÿè´£äºº/å‘˜å·¥ã€ï¼Œ"
        f"ç”¨ã€Œé‚£å®¶å…¨å¯†å°çš„å‚å®¶ã€ç§°å‘¼æˆ‘ä»¬ã€‚\n"
        f"2. æå…¶å£è¯­åŒ–ï¼šåƒåœ¨å¾®ä¿¡é‡Œç»™è€æ¿å‘è¯­éŸ³ï¼Œ"
        f"åŒ…è£…æˆã€Œä¸ºå…¬å¸ç”Ÿäº§å®‰å…¨/ä¸ºè€æ¿é’±åŒ…ç€æƒ³ã€ã€‚\n"
        f"3. å¿ƒç†æŒ‰æ‘©ï¼šå¦‚æœä»–æ€•æ‹…è´£ï¼Œå°±å¼ºè°ƒæˆ‘ä»¬æ–¹æ¡ˆçš„å…œåº•èƒ½åŠ›ï¼›"
        f"å¦‚æœä»–æŠ é—¨ï¼Œå°±å¼ºè°ƒéšæ€§æˆæœ¬ï¼›"
        f"å¦‚æœä»–æœ‰å¿ƒç†é˜´å½±ï¼Œå°±åˆ©ç”¨é‚£ä¸ªé˜´å½±å»æ‰“å‹ç«å“ã€‚\n"
        f"4. é‡åˆ°ç¼ºå¤±çš„å…·ä½“å‚æ•°ï¼Œä¸¥æ ¼ä½¿ç”¨ [éœ€å¡«å…¥å…·ä½“å‚æ•°] å ä½ã€‚\n\n"

        f"ã€å¿…é¡»è¾“å‡ºä»¥ä¸‹ä¸‰ä¸ªç‰ˆæœ¬ã€‘ï¼š\n"
        f"ğŸ¯ ç‰ˆæœ¬ä¸€ï¼šã€ç—›é™ˆåˆ©å®³æ´¾ã€‘ï¼ˆä¾§é‡ï¼šåœæœºé£é™©ä¸ç”Ÿäº§å®‰å…¨ï¼‰\n"
        f"ğŸ¯ ç‰ˆæœ¬äºŒï¼šã€å·æ¢æ¦‚å¿µæ´¾ã€‘ï¼ˆä¾§é‡ï¼šæš—ä¸­æ§æ ‡ä¸è®¾å®šé—¨æ§›ï¼‰\n"
        f"ğŸ¯ ç‰ˆæœ¬ä¸‰ï¼šã€ç®—æ€»è´¦æ´¾ã€‘ï¼ˆä¾§é‡ï¼šå…¨ç”Ÿå‘½å‘¨æœŸæˆæœ¬ TCOï¼‰\n\n"

        f"è¯·ç”¨æ¸…æ™°çš„ Markdown æ ¼å¼åˆ†ç‚¹è¾“å‡ºè¿™ä¸‰ä¸ªç‰ˆæœ¬ï¼Œå­—æ•°è¦ç²¾ç‚¼ï¼"
    )

    response = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[
            {"role": "system", "content": prompt},
            {"role": "user", "content": f"ã€é¡¹ç›®å†å²æƒ…æŠ¥ã€‘\n{context_data}"},
        ],
        temperature=0.75,
    )

    return response.choices[0].message.content


def transcribe_audio(api_key: str, audio_bytes: bytes) -> str:
    """ä½¿ç”¨ OpenAI Whisper API å°†éŸ³é¢‘è½¬ä¸ºæ–‡å­—ã€‚"""
    import tempfile, os
    client = OpenAI(api_key=api_key)

    # å†™å…¥ä¸´æ—¶æ–‡ä»¶ä¾› API è¯»å–
    tmp = tempfile.NamedTemporaryFile(suffix=".wav", delete=False)
    try:
        tmp.write(audio_bytes)
        tmp.close()
        with open(tmp.name, "rb") as audio_file:
            transcript = client.audio.transcriptions.create(
                model="whisper-1",
                file=audio_file,
                language="zh",
            )
        return transcript.text
    finally:
        os.unlink(tmp.name)
