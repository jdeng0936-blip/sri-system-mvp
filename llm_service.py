import base64
import sys
from dataclasses import dataclass

import openai
from openai import OpenAI


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸŒ GlobalLLMRouter â€” é«˜å¯ç”¨å¤§æ¨¡å‹è·¯ç”±ï¼ˆ5 çº§å›é€€é˜²çº¿ï¼‰
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# ANSI é¢œè‰²å¸¸é‡
_YELLOW = "\033[93m"
_RED = "\033[91m"
_GREEN = "\033[92m"
_CYAN = "\033[96m"
_RESET = "\033[0m"
_BOLD = "\033[1m"


@dataclass
class LLMProvider:
    """å•ä¸ª LLM æä¾›å•†é…ç½®"""
    name: str       # "OpenAI" / "Google" / "Anthropic" / "xAI" / "Local"
    model: str      # æ¨¡å‹æ ‡è¯†ç¬¦
    base_url: str   # API base URLï¼ˆæ”¯æŒåå‘ä»£ç†ï¼‰
    api_key: str    # åŠ¨æ€ä¼ å…¥
    timeout: int    # è¶…æ—¶ç§’æ•°


class GlobalLLMRouter:
    """
    é«˜å¯ç”¨å¤§æ¨¡å‹è·¯ç”± â€” 5 çº§å›é€€é˜²çº¿ã€‚
    æŒ‰ä¼˜å…ˆçº§é€ä¸ªå°è¯• providerï¼Œä»»æ„æˆåŠŸå³è¿”å›ï¼Œå…¨éƒ¨å¤±è´¥æŠ›å‡ºå¼‚å¸¸ã€‚
    """

    def __init__(self, providers: list[LLMProvider]):
        self.providers = providers

    def chat(self, messages: list[dict], temperature: float = 0.6, **kwargs) -> str:
        """ç»Ÿä¸€è°ƒç”¨å…¥å£ï¼Œè‡ªåŠ¨å›é€€ã€‚"""
        errors: list[str] = []
        total = len([p for p in self.providers if p.api_key])

        for idx, provider in enumerate(self.providers, 1):
            if not provider.api_key:
                continue  # è·³è¿‡æœªé…ç½® Key çš„ provider

            try:
                print(
                    f"{_CYAN}{_BOLD}ğŸ”— [{idx}/{total}] "
                    f"å°è¯• {provider.name} ({provider.model})...{_RESET}",
                    file=sys.stderr,
                )

                if provider.name == "Anthropic":
                    # ä½¿ç”¨åŸç”Ÿ Anthropic SDK
                    import anthropic
                    client = anthropic.Anthropic(
                        api_key=provider.api_key,
                        timeout=provider.timeout,
                    )
                    # æå– system æ¶ˆæ¯å’Œ user/assistant æ¶ˆæ¯
                    system_text = ""
                    user_msgs = []
                    for m in messages:
                        if m["role"] == "system":
                            system_text += m["content"] + "\n"
                        else:
                            user_msgs.append({"role": m["role"], "content": m["content"]})
                    if not user_msgs:
                        user_msgs = [{"role": "user", "content": system_text}]
                        system_text = ""
                    create_kwargs = {
                        "model": provider.model,
                        "max_tokens": 4096,
                        "messages": user_msgs,
                        "temperature": temperature,
                    }
                    if system_text.strip():
                        create_kwargs["system"] = system_text.strip()
                    response = client.messages.create(**create_kwargs)
                    content = response.content[0].text
                else:
                    # OpenAI å…¼å®¹ SDKï¼ˆOpenAI / Gemini / xAI / Localï¼‰
                    client = OpenAI(
                        api_key=provider.api_key,
                        base_url=provider.base_url,
                        timeout=provider.timeout,
                    )
                    response = client.chat.completions.create(
                        model=provider.model,
                        messages=messages,
                        temperature=temperature,
                    )
                    content = response.choices[0].message.content

                print(
                    f"{_GREEN}{_BOLD}âœ… {provider.name} å‘½ä¸­æˆåŠŸï¼{_RESET}",
                    file=sys.stderr,
                )
                return content

            except openai.AuthenticationError as e:
                msg = f"[{provider.name}] ğŸ”‘ AuthError (Key æ— æ•ˆ): {e}"
                errors.append(msg)
                print(
                    f"{_RED}{_BOLD}â›” {provider.name} Key æ— æ•ˆ (401)ï¼Œ"
                    f"æ­£åœ¨åˆ‡æ¢è‡³ä¸‹ä¸€é˜²çº¿...{_RESET}",
                    file=sys.stderr,
                )
                continue

            except openai.APITimeoutError as e:
                msg = f"[{provider.name}] â±ï¸ Timeout: {e}"
                errors.append(msg)
                print(
                    f"{_YELLOW}{_BOLD}âš ï¸ {provider.name} è°ƒç”¨è¶…æ—¶ ({provider.timeout}s)ï¼Œ"
                    f"æ­£åœ¨æ— ç¼åˆ‡æ¢è‡³ä¸‹ä¸€é˜²çº¿...{_RESET}",
                    file=sys.stderr,
                )
                continue

            except openai.RateLimitError as e:
                msg = f"[{provider.name}] ğŸš¦ RateLimit: {e}"
                errors.append(msg)
                print(
                    f"{_YELLOW}{_BOLD}âš ï¸ {provider.name} è§¦å‘é™æµï¼Œ"
                    f"æ­£åœ¨æ— ç¼åˆ‡æ¢è‡³ä¸‹ä¸€é˜²çº¿...{_RESET}",
                    file=sys.stderr,
                )
                continue

            except (openai.APIConnectionError, openai.InternalServerError) as e:
                msg = f"[{provider.name}] ğŸ’¥ {type(e).__name__}: {e}"
                errors.append(msg)
                print(
                    f"{_RED}{_BOLD}âš ï¸ {provider.name} æœåŠ¡å¼‚å¸¸ ({type(e).__name__})ï¼Œ"
                    f"æ­£åœ¨æ— ç¼åˆ‡æ¢è‡³ä¸‹ä¸€é˜²çº¿...{_RESET}",
                    file=sys.stderr,
                )
                continue

            except openai.BadRequestError as e:
                msg = f"[{provider.name}] âš ï¸ BadRequest (400): {e}"
                errors.append(msg)
                print(
                    f"{_RED}{_BOLD}âš ï¸ {provider.name} è¯·æ±‚è¢«æ‹’ (400: {e})ï¼Œ"
                    f"æ­£åœ¨åˆ‡æ¢è‡³ä¸‹ä¸€é˜²çº¿...{_RESET}",
                    file=sys.stderr,
                )
                continue

            except Exception as e:
                msg = f"[{provider.name}] â“ {type(e).__name__}: {e}"
                errors.append(msg)
                print(
                    f"{_RED}{_BOLD}âš ï¸ {provider.name} æœªçŸ¥å¼‚å¸¸ ({type(e).__name__}: {e})ï¼Œ"
                    f"æ­£åœ¨åˆ‡æ¢è‡³ä¸‹ä¸€é˜²çº¿...{_RESET}",
                    file=sys.stderr,
                )
                continue

        # å…¨éƒ¨å¤±è´¥
        error_detail = "\n".join(errors)
        print(
            f"{_RED}{_BOLD}ğŸš¨ æ‰€æœ‰ LLM é˜²çº¿å‡å·²å¤±è´¥ï¼\n{error_detail}{_RESET}",
            file=sys.stderr,
        )
        raise RuntimeError(f"æ‰€æœ‰ LLM é˜²çº¿å‡å·²å¤±è´¥:\n{error_detail}")


def build_llm_router(
    primary_api_key: str = "",
    llm_configs: dict | None = None,
) -> GlobalLLMRouter:
    """
    æŒ‰ä¼˜å…ˆçº§æ„å»º 5 çº§è·¯ç”±ã€‚
    å¦‚æœå‰ç«¯ä¼ å…¥äº† llm_configsï¼Œä½¿ç”¨åŠ¨æ€é…ç½®ï¼›å¦åˆ™é™çº§ä¸ºå• Key æ¨¡å¼ã€‚
    ä¸¥æ ¼æ£€æŸ¥ enabled å­—æ®µï¼Œç¦ç”¨çš„ provider ä¸è¿›å…¥è·¯ç”±ã€‚
    """
    cfg = llm_configs or {}

    # è¾…åŠ©å‡½æ•°ï¼šè¯»å–åŸå§‹é…ç½®å¹¶å¡«å……é»˜è®¤å€¼
    def _get(provider_key: str, field: str, default: str) -> str:
        return cfg.get(provider_key, {}).get(field, "") or default

    def _enabled(provider_key: str, default: bool) -> bool:
        p = cfg.get(provider_key, {})
        if isinstance(p, dict) and "enabled" in p:
            return bool(p["enabled"])
        return default

    providers: list[LLMProvider] = []

    # ç¬¬ä¸€é˜²çº¿: OpenAI
    if _enabled("openai", True):
        key = _get("openai", "apiKey", primary_api_key)
        if key:
            providers.append(LLMProvider(
                name="OpenAI",
                model=_get("openai", "model", "gpt-4o-mini"),
                base_url=_get("openai", "baseUrl", "https://api.openai.com/v1"),
                api_key=key,
                timeout=30,
            ))

    # ç¬¬äºŒé˜²çº¿: Google Gemini
    if _enabled("gemini", False):
        key = _get("gemini", "apiKey", "")
        if key:
            providers.append(LLMProvider(
                name="Google Gemini",
                model=_get("gemini", "model", "gemini-2.0-flash"),
                base_url=_get("gemini", "baseUrl", "https://generativelanguage.googleapis.com/v1beta/openai/"),
                api_key=key,
                timeout=30,
            ))

    # ç¬¬ä¸‰é˜²çº¿: Anthropic
    if _enabled("anthropic", False):
        key = _get("anthropic", "apiKey", "")
        if key:
            providers.append(LLMProvider(
                name="Anthropic",
                model=_get("anthropic", "model", "claude-3-5-sonnet-20241022"),
                base_url=_get("anthropic", "baseUrl", "https://api.anthropic.com/v1/"),
                api_key=key,
                timeout=45,
            ))

    # ç¬¬å››é˜²çº¿: xAI Grok
    if _enabled("xai", False):
        key = _get("xai", "apiKey", "")
        if key:
            providers.append(LLMProvider(
                name="xAI Grok",
                model=_get("xai", "model", "grok-3-mini"),
                base_url=_get("xai", "baseUrl", "https://api.x.ai/v1"),
                api_key=key,
                timeout=30,
            ))

    # ç»ˆæç‰©ç†é˜²çº¿: Local DeepSeekï¼ˆé»˜è®¤å¯ç”¨ï¼Œå¯é€šè¿‡é…ç½®ç¦ç”¨ï¼‰
    if _enabled("local", True):
        local_url = _get("local", "baseUrl", "http://localhost:11434/v1")
        providers.append(LLMProvider(
            name="Local DeepSeek",
            model=_get("local", "model", "deepseek-r1"),
            base_url=local_url,
            api_key="local",
            timeout=120,
        ))

    return GlobalLLMRouter(providers)


SYSTEM_PROMPT = (
    "ä½ æ˜¯ä¸€åèµ„æ·±å·¥ä¸šç”µæ°”é”€å”®ä¸“å®¶ã€‚è¯·å¯¹é”€å”®æ‹œè®¿å£è¿°è®°å½•è¿›è¡Œç»“æ„åŒ–æƒ…æŠ¥æå–ï¼Œ"
    "ä¸¥æ ¼è¿”å›ä»¥ä¸‹ JSON æ ¼å¼ï¼ˆ4+1 æƒ…æŠ¥æ¨¡å‹ï¼‰ï¼š\n"
    "{\n"
    "  \"current_status\": \"é¡¹ç›®ç°çŠ¶ã€é¢„ç®—ä¸è¿›åº¦ä¿¡æ¯\",\n"
    "  \"decision_chain\": [\n"
    "    {\"name\": \"å§“å\", \"title\": \"èŒåŠ¡\", \"phone\": \"è”ç³»æ–¹å¼(è‹¥æ— åˆ™è¿”å›null)\", "
    "\"attitude\": \"æ”¯æŒ/ä¸­ç«‹/åå¯¹\", \"soft_tags\": [\"æ ‡ç­¾1\", \"æ ‡ç­¾2\"]}\n"
    "  ],\n"
    "  \"competitor_info\": [\n"
    "    {\"name\": \"ç«å“åç§°\", \"quote\": \"æŠ¥ä»·(è‹¥æ— åˆ™è¿”å›null)\", "
    "\"strengths\": \"ä¼˜åŠ¿\", \"weaknesses\": \"åŠ£åŠ¿\", \"recent_actions\": \"è¿‘æœŸåŠ¨ä½œ\"}\n"
    "  ],\n"
    "  \"next_steps\": \"ä¸‹ä¸€æ­¥è¡ŒåŠ¨è®¡åˆ’æˆ–é”€å”®æ‰¿è¯º\",\n"
    "  \"gap_alerts\": [\"ç¼ºå£é¢„è­¦1\", \"ç¼ºå£é¢„è­¦2\"]\n"
    "}\n\n"
    "ä½œä¸ºä¸¥è‹›çš„é”€å”®æ€»ç›‘ï¼Œè¯·å®¡æŸ¥æ‹œè®¿è®°å½•å¹¶åœ¨ gap_alerts ä¸­æŒ‡å‡ºç¼ºå¤±çš„è‡´å‘½æƒ…æŠ¥ã€‚è§„åˆ™ï¼š\n"
    "1. æåˆ°äººç‰©ä½†æœªæä¾›ç”µè¯æˆ–è”ç³»æ–¹å¼ â†’ 'âš ï¸ æœªè·å– [å§“å] çš„è”ç³»æ–¹å¼'ã€‚\n"
    "2. æœªæåˆ°ä¸‹ä¸€æ­¥æ¨è¿›æ—¶é—´ â†’ 'âš ï¸ ç¼ºå°‘æ˜ç¡®çš„ä¸‹ä¸€æ­¥æ¨è¿›æ—¶é—´ç‚¹'ã€‚\n"
    "3. æœªç¡®è®¤é¡¹ç›®é¢„ç®— â†’ 'âš ï¸ æœªç¡®è®¤æœ€ç»ˆé¢„ç®—'ã€‚\n"
    "4. æœªè¯†åˆ«å…³é”®å†³ç­–äºº â†’ 'âš ï¸ æœªè¯†åˆ«å…³é”®å†³ç­–äºº'ã€‚\n"
    "å¦‚æœæƒ…æŠ¥å®Œç¾ï¼Œgap_alerts è¿”å›ç©ºæ•°ç»„ []ã€‚\n\n"
    "ä¸¥ç¦è¾“å‡ºä»»ä½• Markdown æ ‡è®°æˆ–å¤šä½™çš„è§£é‡Šè¯´æ˜ï¼Œåªè¿”å›åˆæ³•çš„ JSON å­—ç¬¦ä¸²ã€‚"
)


def encode_image(uploaded_file) -> str:
    """å°†ä¸Šä¼ çš„å›¾ç‰‡æ–‡ä»¶è½¬æ¢ä¸º Base64 å­—ç¬¦ä¸²ã€‚"""
    file_bytes = uploaded_file.read()
    uploaded_file.seek(0)  # é‡ç½®æŒ‡é’ˆä»¥ä¾¿å…¶ä»–åœ°æ–¹ç»§ç»­è¯»å–
    return base64.b64encode(file_bytes).decode("utf-8")


def parse_visit_log(api_key: str, raw_text: str) -> str:
    """è°ƒç”¨å¤§æ¨¡å‹ï¼Œå°†æ‹œè®¿æµæ°´è´¦æç‚¼ä¸ºç»“æ„åŒ– JSONï¼ˆ4+1 æƒ…æŠ¥æ¨¡å‹ï¼‰ã€‚"""
    # æ ¹æ® key å‰ç¼€è‡ªåŠ¨æ„å»ºè·¯ç”±
    llm_configs = _detect_llm_config(api_key)
    router = build_llm_router(primary_api_key=api_key, llm_configs=llm_configs)

    return router.chat(
        messages=[
            {"role": "system", "content": SYSTEM_PROMPT},
            {"role": "user", "content": raw_text},
        ],
        temperature=0.2,
    )


def _detect_llm_config(api_key: str) -> dict:
    """æ ¹æ® API Key å‰ç¼€è‡ªåŠ¨æ£€æµ‹ LLM æä¾›å•†å¹¶æ„å»ºé…ç½®ã€‚"""
    if api_key.startswith("sk-ant-"):
        return {
            "openai": {"enabled": False},
            "anthropic": {"enabled": True, "apiKey": api_key, "model": "claude-3-5-sonnet-20241022"},
            "local": {"enabled": False},  # ç¦ç”¨æœ¬åœ°å›é€€ï¼Œé¿å…å¡ä½
        }
    # é»˜è®¤èµ° OpenAI
    return {}


def parse_visit_log_with_image(api_key: str, raw_text: str, image_base64: str) -> str:
    """è°ƒç”¨å¤šæ¨¡æ€å¤§æ¨¡å‹ï¼ŒåŒæ—¶è§£ææ–‡å­—å£è¿° + å›¾ç‰‡æƒ…æŠ¥ï¼Œè¾“å‡º 4+1 JSONã€‚"""
    client = OpenAI(api_key=api_key)

    user_content = [
        {
            "type": "text",
            "text": (
                "ä»¥ä¸‹æ˜¯é”€å”®çš„æ–‡å­—å£è¿°è®°å½•ï¼Œè¯·ç»“åˆå£è¿°å†…å®¹å’Œå›¾ç‰‡ä¸­çš„ä¿¡æ¯"
                "ï¼ˆå¦‚åç‰‡ä¸Šçš„å§“å/ç”µè¯ã€è®¾å¤‡é“­ç‰Œå‚æ•°ã€æŠ¥ä»·å•ä»·æ ¼ç­‰ï¼‰ï¼Œ"
                "åˆå¹¶åä¸¥æ ¼æŒ‰ç…§ 4+1 JSON æ ¼å¼è¾“å‡ºã€‚\n\n"
                f"ã€é”€å”®å£è¿°è®°å½•ã€‘\n{raw_text}"
            ),
        },
        {
            "type": "image_url",
            "image_url": {
                "url": f"data:image/jpeg;base64,{image_base64}",
                "detail": "high",
            },
        },
    ]

    response = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[
            {"role": "system", "content": SYSTEM_PROMPT},
            {"role": "user", "content": user_content},
        ],
        temperature=0.2,
    )

    return response.choices[0].message.content


ADVISOR_PROMPT = (
    "ä½ æ˜¯ä¸€åç‹ è¾£çš„å·¥ä¸šé”€å”®å†›å¸ˆã€‚è¯·æ ¹æ®æä¾›çš„é¡¹ç›®å†å²è®°å½•ï¼Œå›ç­”ç”¨æˆ·é—®é¢˜ã€‚"
    "å¦‚æœæ˜¯å†™å‘¨æŠ¥ï¼Œè¯·ç”¨æå…¶ä¸“ä¸šçš„å•†åŠ¡é£æ ¼ï¼›å¦‚æœæ˜¯åˆ†æå±€åŠ¿ï¼Œè¯·ç›´è¨€ä¸è®³åœ°æŒ‡å‡ºé£é™©ã€‚"
    "å›ç­”å¿…é¡»åŸºäºå®¢è§‚æƒ…æŠ¥ï¼Œç»™å‡ºçŠ€åˆ©ã€ä¸“ä¸šçš„åˆ†æå’Œç­–ç•¥å»ºè®®ã€‚"
)


def chat_with_project(api_key: str, context_data: str, user_query: str) -> str:
    """åŸºäºé¡¹ç›®æƒ…æŠ¥ä¸Šä¸‹æ–‡ï¼Œä¸ AI å‚è°‹å¯¹è¯ï¼ˆéæµå¼ï¼‰ã€‚"""
    client = OpenAI(api_key=api_key)

    user_message = (
        f"ã€é¡¹ç›®å†å²æƒ…æŠ¥ä¸Šä¸‹æ–‡ã€‘\n{context_data}\n\n"
        f"ã€æˆ‘çš„é—®é¢˜ã€‘\n{user_query}"
    )

    response = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[
            {"role": "system", "content": ADVISOR_PROMPT},
            {"role": "user", "content": user_message},
        ],
        temperature=0.5,
    )

    return response.choices[0].message.content


def chat_with_project_stream(api_key: str, context_data: str, messages: list):
    """åŸºäºé¡¹ç›®æƒ…æŠ¥ä¸Šä¸‹æ–‡ï¼Œä¸ AI å‚è°‹å¯¹è¯ï¼ˆæµå¼è¾“å‡ºï¼‰ã€‚
    
    messages: å®Œæ•´çš„å¯¹è¯å†å² [{"role": "user"/"assistant", "content": "..."}]
    è¿”å›ä¸€ä¸ªç”Ÿæˆå™¨ï¼Œé€ chunk yield æ–‡æœ¬ã€‚
    """
    client = OpenAI(api_key=api_key)

    # å°†é¡¹ç›®æƒ…æŠ¥æ³¨å…¥ system prompt
    system_msg = (
        f"{ADVISOR_PROMPT}\n\n"
        f"ã€å½“å‰é¡¹ç›®çš„å…¨éƒ¨å†å²æƒ…æŠ¥æ•°æ®ã€‘\n{context_data}"
    )

    api_messages = [{"role": "system", "content": system_msg}] + messages

    stream = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=api_messages,
        temperature=0.5,
        stream=True,
    )

    for chunk in stream:
        delta = chunk.choices[0].delta
        if delta.content:
            yield delta.content


def generate_quiz(api_key: str, context_data: str, blind_spots: str = "æ— ") -> str:
    """åŸºäºé¡¹ç›®æƒ…æŠ¥ + å†å²ç›²ç‚¹ï¼Œç”Ÿæˆä¸€é“ä¸‰ç»´å®æˆ˜æƒ…æ™¯æ¨¡æ‹Ÿæµ‹éªŒé¢˜ã€‚"""
    client = OpenAI(api_key=api_key)

    coach_prompt = (
        "ä½ æ˜¯ä¸€åé¡¶çº§çš„å·¥ä¸šç”µæ°”é”€å”®æ•™ç»ƒå…¼æŠ€æœ¯æ€»å·¥ã€‚\n"
        "è¯·ä»”ç»†é˜…è¯»è¯¥é”€å”®çš„é¡¹ç›®æ±‡æŠ¥å†…å®¹ï¼Œä»¥åŠä»–è¿‡å»çš„"
        f"ã€çŸ¥è¯†ç›²ç‚¹ï¼š{blind_spots}ã€‘ã€‚\n"
        "ä½ éœ€è¦ä»ä»¥ä¸‹ä¸‰ä¸ªç»´åº¦ä¸­ï¼Œé’ˆå¯¹ä»–çš„ç›²ç‚¹æˆ–å½“å‰é¡¹ç›®çš„è–„å¼±ç¯èŠ‚ï¼Œ"
        "æå‡ºä¸€é“æå…¶åˆé’»çš„å®æˆ˜æµ‹éªŒé¢˜ï¼š\n\n"
        "1. å•†åŠ¡åšå¼ˆï¼ˆå¦‚ï¼šåº”å¯¹ç«å“ä½ä»·ã€æå®šå…³é”®å†³ç­–äººã€æ¨è¿›é‡‡è´­æµç¨‹ï¼‰ã€‚\n"
        "2. æŠ€æœ¯æ–¹æ¡ˆï¼ˆå¦‚ï¼šé«˜ä½å‹æŸœæ ¸å¿ƒå‚æ•°è§£æã€æˆ‘æ–¹æ–¹æ¡ˆçš„ç”µæ°”æŠ€æœ¯ä¼˜åŠ¿ã€"
        "è§£ç­”å®¢æˆ·å¯¹å…ç»´æŠ¤è®¾è®¡çš„æŠ€æœ¯è´¨ç–‘ï¼‰ã€‚\n"
        "3. è¡Œä¸šè®¤çŸ¥ï¼ˆå¦‚ï¼šè¯¥å®¢æˆ·æ‰€åœ¨è¡Œä¸šçš„æœ€æ–°æ”¿ç­–å¯¼å‘ã€å…¸å‹ç”¨ç”µè´Ÿè·ç‰¹å¾ã€"
        "ä¸Šä¸‹æ¸¸ç—›ç‚¹ï¼‰ã€‚\n\n"
        "è¦æ±‚ï¼š\n"
        "- ä¸è¦è¯´åºŸè¯ï¼Œç»“åˆé¡¹ç›®å½“å‰çœŸå®æƒ…å†µç›´æ¥æé—®ã€‚\n"
        "- é—®é¢˜å¿…é¡»å…·æœ‰æå¼ºçš„å‹è¿«æ„Ÿå’Œå®æˆ˜æ„ä¹‰ã€‚\n"
        "- å­—æ•°ä¸¥æ ¼æ§åˆ¶åœ¨ 100 å­—ä»¥å†…ã€‚"
    )

    response = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[
            {"role": "system", "content": coach_prompt},
            {"role": "user", "content": f"ã€é¡¹ç›®å†å²æƒ…æŠ¥ã€‘\n{context_data}"},
        ],
        temperature=0.8,
    )

    return response.choices[0].message.content


CRITIQUE_PROMPT = (
    "ä½ æ˜¯ä¸€åæå…¶ä¸¥è‹›ã€ç‹ è¾£çš„å·¥ä¸šé”€å”®æ€»ç›‘ã€‚è¯·è¯„ä¼°é”€å”®äººå‘˜å¯¹å®æˆ˜é—®é¢˜çš„å›ç­”ã€‚\n"
    "å¦‚æœå›ç­”æ˜¯\"åšå¥½å®¢æƒ…\"ã€\"åŠ å¼ºæ²Ÿé€š\"ç­‰å‡å¤§ç©ºçš„åºŸè¯ï¼Œè¯·ç»™äºˆæä½çš„åˆ†æ•°å¹¶ä¸¥å‰æ‰¹è¯„ã€‚\n"
    "ä½ å¿…é¡»ä¸¥æ ¼è¿”å›ä»¥ä¸‹ JSON æ ¼å¼ï¼š\n"
    "{\n"
    "  \"score\": æ•°å­— (0-100çš„è¯„åˆ†),\n"
    "  \"critique\": \"ä¸¥å‰ä¸”ç›´æŒ‡æ ¸å¿ƒçš„ç‚¹è¯„è¯ï¼Œä¸è¶…è¿‡150å­—\",\n"
    "  \"blind_spots\": [\"ç›²ç‚¹æ ‡ç­¾1\", \"ç›²ç‚¹æ ‡ç­¾2\"] "
    "(æå–1-3ä¸ªä»–ç¼ºä¹çš„è®¤çŸ¥ç›²ç‚¹ï¼Œå¦‚\"ç¼ºä¹å…·ä½“æŠ€æœ¯åé©³è¯æœ¯\"ã€\"å¯¹ç«å“äº¤æœŸæ— é¢„æ¡ˆ\")\n"
    "}\n\n"
    "ä¸¥ç¦è¾“å‡ºä»»ä½• Markdown æ ‡è®°æˆ–å¤šä½™çš„è§£é‡Šè¯´æ˜ï¼Œåªè¿”å›åˆæ³•çš„ JSON å­—ç¬¦ä¸²ã€‚"
)


def critique_answer(api_key: str, quiz: str, user_answer: str) -> str:
    """è¯„ä¼°é”€å”®äººå‘˜çš„åº”å¯¹ç­–ç•¥ï¼Œè¿”å› JSON æ ¼å¼çš„è¯„åˆ†ã€ç‚¹è¯„å’Œç›²ç‚¹ã€‚"""
    client = OpenAI(api_key=api_key)

    user_message = (
        f"ã€æµ‹éªŒé¢˜ç›®ã€‘\n{quiz}\n\n"
        f"ã€é”€å”®äººå‘˜çš„å›ç­”ã€‘\n{user_answer}"
    )

    response = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[
            {"role": "system", "content": CRITIQUE_PROMPT},
            {"role": "user", "content": user_message},
        ],
        temperature=0.3,
    )

    return response.choices[0].message.content


TEAM_REPORT_PROMPT = (
    "ä½ æ˜¯ä¸€åé¡¶çº§çš„å·¥ä¸šç”µæ°”é”€å”®èµ‹èƒ½ä¸“å®¶ã€‚\n"
    "è¯·æ ¹æ®ä»¥ä¸‹æ±‡æ€»çš„ã€å›¢é˜Ÿè¿‘æœŸæš´éœ²çš„çŸ¥è¯†ç›²ç‚¹ã€‘ï¼Œè¿›è¡Œæ·±åº¦å½’çº³åˆ†æã€‚\n"
    "è¯·è¾“å‡ºä¸€ä»½ç®€æ˜æ‰¼è¦çš„ä½“æ£€æŠ¥å‘Šï¼Œå¿…é¡»åŒ…å«ï¼š\n"
    "1. æ ¸å¿ƒå…±æ€§çŸ­æ¿ï¼ˆå¤§å®¶æ™®éæ¬ ç¼ºçš„3ä¸ªèƒ½åŠ›ï¼‰ã€‚\n"
    "2. ä¸‹ä¸€æ­¥é’ˆå¯¹æ€§åŸ¹è®­å»ºè®®ï¼ˆå…·ä½“åˆ°åº”è¯¥è¡¥é½ä»€ä¹ˆè¯æœ¯æˆ–æŠ€æœ¯çŸ¥è¯†ï¼‰ã€‚\n"
    "ä¸è¦åºŸè¯ï¼Œç›´æ¥è¾“å‡ºä¸“ä¸šåˆ†æã€‚"
)


def generate_team_report(api_key: str, blind_spots_summary: str) -> str:
    """åŸºäºå›¢é˜Ÿç›²ç‚¹æ±‡æ€»æ•°æ®ï¼Œç”Ÿæˆå›¢é˜Ÿèƒ½åŠ›ä½“æ£€æŠ¥å‘Šã€‚"""
    client = OpenAI(api_key=api_key)

    response = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[
            {"role": "system", "content": TEAM_REPORT_PROMPT},
            {"role": "user", "content": f"ã€å›¢é˜Ÿè¿‘æœŸæš´éœ²çš„çŸ¥è¯†ç›²ç‚¹æ±‡æ€»ã€‘\n{blind_spots_summary}"},
        ],
        temperature=0.5,
    )

    return response.choices[0].message.content


FOLLOWUP_PROMPTS_EMAIL = (
    'ä½ æ˜¯ä¸€åæå…¶ä¸“ä¸šçš„å·¥ä¸šå¤§å®¢æˆ·é”€å”®æ€»ç›‘ã€‚\n'
    'è¯·æ ¹æ®æä¾›çš„ã€é¡¹ç›®å†å²æƒ…æŠ¥ã€‘ï¼Œä¸ºå‰çº¿é”€å”®å†™ä¸€å°å‘ç»™è¯¥é¡¹ç›®æ ¸å¿ƒå†³ç­–äººçš„è·Ÿè¿›é‚®ä»¶ã€‚\n'
    'è¦æ±‚ï¼š\n'
    '1. å¿…é¡»åŒ…å«é‚®ä»¶æ ‡é¢˜è¡Œï¼ˆSubject:ï¼‰ã€‚\n'
    '2. ç§°å‘¼å¿…é¡»å‡†ç¡®ï¼ˆä»æƒ…æŠ¥ä¸­æå–æœ€é«˜çº§åˆ«çš„å†³ç­–äººï¼‰ã€‚\n'
    '3. é‚®ä»¶æ­£æ–‡ç›´å‡»å®¢æˆ·ç—›ç‚¹ï¼Œå·§å¦™æš—ç¤ºæˆ‘ä»¬åœ¨æŠ€æœ¯æˆ–äº¤æœŸä¸Šä¼˜äºç«äº‰å¯¹æ‰‹ï¼Œ'
    'ä½†ä¸å¯å¸¦æœ‰æ˜æ˜¾æ”»å‡»æ€§ã€‚\n'
    '4. å¿…é¡»åŒ…å«ä¸€ä¸ªæ˜ç¡®çš„ä¸‹ä¸€æ­¥æ¨è¿›åŠ¨ä½œï¼ˆä¾‹å¦‚è¯·æ±‚ä¸‹å‘¨ä¸‰è¿›è¡ŒæŠ€æœ¯æ±‡æŠ¥ï¼‰ã€‚\n'
    '5. ç»“å°¾å¿…é¡»æœ‰ä¸“ä¸šè½æ¬¾ã€‚\n'
    '6. è¯­æ°”ä¸“ä¸šã€è¯šæ³ã€ä¸å‘ä¸äº¢ã€‚'
)


def _build_wechat_followup_prompt(target_person: str) -> str:
    return (
        f'ä½ æ˜¯ä¸€åæ·±è°™äººæƒ…ä¸–æ•…çš„é¡¶çº§å·¥ä¸šé”€å”®ã€‚\n'
        f'è¯·ä¸ºå½“å‰é¡¹ç›®å‘ã€{target_person}ã€‘å†™ä¸€æ¡å¾®ä¿¡è·Ÿè¿›æ¶ˆæ¯ã€‚\n\n'
        f'è¯·é¦–å…ˆä»”ç»†é˜…è¯»ä»¥ä¸‹ã€é¡¹ç›®å†å²æƒ…æŠ¥ã€‘ï¼Œé‡ç‚¹åˆ†æã€{target_person}ã€‘çš„'
        f'èŒåŠ¡ã€ç—›ç‚¹ã€æ€§æ ¼æ ‡ç­¾ï¼Œä»¥åŠæˆ‘æ–¹ç›®å‰ä¸ä»–çš„ã€å…³ç³»æ·±åº¦ã€‘ã€‚\n\n'
        f'ã€å¼ºåˆ¶æ€§å®šåˆ¶åŒ–åŸåˆ™ã€‘ï¼š\n'
        f'1. çœ‹äººä¸‹èœç¢Ÿï¼ˆåŸºäºè§’è‰²ï¼‰ï¼š\n'
        f'   - å¦‚æœæ˜¯æŠ€æœ¯/æ€»å·¥ï¼Œå¤šæå…ç»´æŠ¤ã€ç¨³å®šæ€§ã€æŠ€æœ¯éªŒè¯ï¼›\n'
        f'   - å¦‚æœæ˜¯é‡‡è´­/å•†åŠ¡ï¼Œå¤šæäº¤æœŸä¿éšœã€ç»¼åˆæˆæœ¬ä¼˜åŠ¿ï¼›\n'
        f'   - å¦‚æœæ˜¯å¤§è€æ¿/å†³ç­–äººï¼Œå¤šææ— å¿§è¿è¡Œã€å¤§å±€è§‚ã€é•¿æœŸåˆä½œä»·å€¼ã€‚\n'
        f'2. æŠŠæ¡åˆ†å¯¸æ„Ÿï¼ˆåŸºäºå…³ç³»æ·±åº¦ï¼‰ï¼š\n'
        f'   - å¦‚æœæƒ…æŠ¥æ˜¾ç¤ºè¿˜æ²¡åŠ ä¸Šå¾®ä¿¡æˆ–åˆšæ¥è§¦ï¼Œè¯­æ°”å¿…é¡»ä¸“ä¸šã€è°¦é€Šã€ç›´æ¥è¡¨æ˜ä»·å€¼ï¼›\n'
        f'   - å¦‚æœæƒ…æŠ¥æ˜¾ç¤ºå·²ç»æ˜¯ç†Ÿäºº/å†…çº¿ï¼ˆå¦‚ç»å¸¸æ‹œè®¿ã€é€éœ²äº†ç«å“åº•ä»·ï¼‰ï¼Œ'
        f'è¯­æ°”è¦æå…¶è‡ªç„¶ã€å£è¯­åŒ–ï¼Œç”šè‡³å¯ä»¥å¸¦ç‚¹è°ƒä¾ƒã€‚\n'
        f'3. åƒçœŸäººä¸€æ ·è‡ªç„¶ï¼Œç¦æ­¢ç¿»è¯‘è…”ã€‚\n'
        f'4. å¼€å¤´ç›´æ¥ç‚¹æ˜äº‹ç”±ï¼Œä¸­é—´è½¯æ¤å…¥æˆ‘æ–¹ä¼˜åŠ¿ï¼Œç»“å°¾ç•™ä¸€ä¸ªè½»æ¾çš„äº’åŠ¨é’©å­ã€‚\n'
        f'5. å­—æ•°æ§åˆ¶åœ¨ 150 å­—ä»¥å†…ï¼Œé€‚å½“ä½¿ç”¨ emojiã€‚'
    )


def generate_followup_email(api_key: str, context_data: str,
                            channel: str = "email",
                            target_person: str = "å…³é”®å†³ç­–äºº",
                            project_stage: str = "åˆæœŸæ¥è§¦",
                            use_top_to_top: bool = False,
                            shared_history: str = "",
                            is_director: bool = False,
                            subordinate_name: str = "") -> str:
    client = OpenAI(api_key=api_key)

    if channel == "wechat":
        prompt = _build_wechat_followup_prompt(target_person)
    else:
        prompt = FOLLOWUP_PROMPTS_EMAIL

    # é˜¶æ®µæ„ŸçŸ¥æ³¨å…¥
    prompt += (
        f"\nã€å½“å‰é¡¹ç›®é˜¶æ®µã€‘ï¼š{project_stage}ã€‚"
        f"è¯·åŠ¡å¿…æ ¹æ®è¯¥é˜¶æ®µçš„ç‰¹å¾è°ƒæ•´è¯æœ¯ã€‚"
        f"ä¾‹å¦‚ï¼šåˆæœŸé‡åœ¨å±•ç¤ºä¸“ä¸šä¸ç ´å†°ï¼›æŠ¥ä»·æœŸé‡åœ¨ä¼ é€’ä»·å€¼ï¼›"
        f"å•†åŠ¡åƒµæŒæœŸé‡åœ¨æ‰“ç ´ä¿¡æ¯å·®æˆ–æ–½å‹ã€‚"
    )

    # é«˜ç®¡ååŒæ³¨å…¥
    if use_top_to_top:
        prompt += (
            "\nã€é«˜ç®¡ååŒæˆ˜ç•¥ã€‘ï¼šå¯¹æ–¹æ˜¯å…¬å¸é•¿çº¿å®¢æˆ·ï¼Œ"
            "è¯·åœ¨è¯æœ¯ä¸­æå…¶å·§å¦™ã€è‡ªç„¶åœ°å¼•å…¥æˆ‘æ–¹é«˜ç®¡/è€é¢†å¯¼ã€‚"
            "ä¸è¦ç”Ÿç¡¬ï¼Œè¦ä½“ç°å‡ºæˆ‘æ–¹é«˜ç®¡å¯¹è¯¥é¡¹ç›®çš„æåº¦é‡è§†ï¼Œ"
            "æˆ–è€…æš—ç¤ºæˆ‘æ–¹é«˜ç®¡ä¸å¯¹æ–¹æœ‰å†å²åˆä½œæ¸Šæº/æ„Ÿæƒ…åŸºç¡€ã€‚"
            "ç›®çš„æ˜¯é€šè¿‡å€ŸåŠ›æ‰“åŠ›ï¼Œä¿ƒæˆä¸‹ä¸€æ­¥çš„é«˜å±‚ä¼šé¢æˆ–è·¨è¿‡åŸºå±‚é˜»ç¢ã€‚"
        )
        if shared_history.strip():
            prompt += (
                f"\nã€å†å²æ¸Šæºç´ æã€‘ï¼š{shared_history.strip()}ã€‚"
                f"è¯·å°†è¿™æ®µå…±åŒç»å†è‡ªç„¶åœ°ç»‡å…¥è¯æœ¯ä¸­ï¼Œ"
                f"å”¤é†’å¯¹æ–¹çš„æƒ…ç»ªè®°å¿†ï¼Œæ‹‰è¿‘è·ç¦»ï¼Œä½†ä¸è¦æ˜¾å¾—åˆ»æ„ç…½æƒ…ã€‚"
            )

    # æ€»ç›‘åŠ©é”€æ¨¡å¼æ³¨å…¥
    if is_director:
        sub = subordinate_name.strip() or "æˆ‘ä»¬çš„é¡¹ç›®è´Ÿè´£äºº"
        prompt += (
            f"\nã€æ€»ç›‘åŠ©é”€æ¨¡å¼ã€‘ï¼šå½“å‰å‘é€è€…èº«ä»½æ˜¯é”€å”®æ€»ç›‘/é«˜ç®¡ï¼Œä¸æ˜¯æ™®é€šé”€å”®ã€‚"
            f"è¯·ä»¥é«˜ç®¡è§†è§’æ’°å†™è¯æœ¯ï¼Œä½“ç°é«˜å±‚äº²è‡ªå…³æ³¨çš„åˆ†é‡æ„Ÿã€‚"
            f"è¯æœ¯ä¸­éœ€è‡ªç„¶åœ°æåŠä¸‹å±ã€{sub}ã€‘ï¼Œ"
            f"ä¾‹å¦‚ï¼š'{sub}è·Ÿæˆ‘æ±‡æŠ¥äº†è´µå¸é¡¹ç›®çš„è¿›å±•ï¼Œæˆ‘éå¸¸é‡è§†...' "
            f"æˆ– 'æˆ‘ç‰¹æ„è®©{sub}æŠŠæœ€æ–°æ–¹æ¡ˆç»™æ‚¨åŒæ­¥ä¸€ä¸‹...' "
            f"ç›®çš„æ˜¯é€šè¿‡é«˜ç®¡èº«ä»½è¿›è¡Œé™ç»´æ‰“å‡»ï¼ŒåŒæ—¶æŠ¬é«˜ä¸‹å±åœ¨å®¢æˆ·å¿ƒä¸­çš„åœ°ä½ã€‚"
        )

    response = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[
            {"role": "system", "content": prompt},
            {"role": "user", "content": f"ã€é¡¹ç›®å†å²æƒ…æŠ¥ã€‘\n{context_data}"},
        ],
        temperature=0.7 if channel == "wechat" else 0.6,
    )

    return response.choices[0].message.content


TECH_PROMPTS = {
    "wechat": (
        'ä½ æ˜¯ä¸€åèµ„æ·±çš„å·¥ä¸šç”µæ°”æŠ€æœ¯æ€»å·¥ã€‚\n'
        'è¯·æ ¹æ®ã€é¡¹ç›®å†å²æƒ…æŠ¥ã€‘ï¼Œç”Ÿæˆä¸€æ®µé€‚åˆåœ¨æ‰‹æœºä¸Šé˜…è¯»çš„æŠ€æœ¯è¦ç‚¹åˆ—è¡¨ã€‚\n'
        'è¦æ±‚ï¼š\n'
        '1. æ¯è¡Œä¸è¶…è¿‡ 15 å­—ï¼Œç”¨ âœ… ç­‰ç¬¦å·åˆ†ç‚¹ã€‚\n'
        '2. çªå‡ºæˆ‘æ–¹æ–¹æ¡ˆçš„æ ¸å¿ƒæŠ€æœ¯äº®ç‚¹å’Œå…ç»´æŠ¤ä¼˜åŠ¿ã€‚\n'
        '3. å¯¹äºå…·ä½“çš„è®¾å¤‡å‚æ•°ï¼ˆå¦‚å°ºå¯¸ã€ç”µæµï¼‰ï¼Œå¦‚æœä½ åœ¨å†å²æƒ…æŠ¥ä¸­æ²¡çœ‹åˆ°ï¼Œ'
        'è¯·ä½¿ç”¨ [éœ€å¡«å…¥æˆ‘æ–¹å…·ä½“å‚æ•°] å ä½ï¼Œä¸è¦çç¼–ã€‚'
    ),
    "email": (
        'ä½ æ˜¯ä¸€åèµ„æ·±çš„å·¥ä¸šç”µæ°”æŠ€æœ¯æ€»å·¥ã€‚\n'
        'è¯·æ ¹æ®æä¾›çš„ã€é¡¹ç›®å†å²æƒ…æŠ¥ã€‘ï¼Œç”Ÿæˆä¸€ä»½ç²¾ç‚¼çš„æŠ€æœ¯æ–¹æ¡ˆæ‘˜è¦ï¼Œ'
        'ç”¨äºæäº¤ç»™å®¢æˆ·çš„æŠ€æœ¯è¯„å®¡å›¢ã€‚\n'
        'è¦æ±‚ï¼š\n'
        '1. ç½—åˆ—æˆ‘æ–¹è®¾å¤‡çš„ç¡¬æ ¸æŠ€æœ¯å‚æ•°ä¸å…ç»´æŠ¤è®¾è®¡ä¼˜åŠ¿ã€‚\n'
        '2. é’ˆå¯¹å®¢æˆ·é¡¹ç›®éœ€æ±‚é€æ¡åŒ¹é…æˆ‘æ–¹æ–¹æ¡ˆçš„æŠ€æœ¯äº®ç‚¹ã€‚\n'
        '3. å¦‚æœ‰ç«å“æƒ…æŠ¥ï¼Œä»¥å¯¹æ¯”è¡¨æ ¼å½¢å¼çªå‡ºå·®å¼‚åŒ–ä¼˜åŠ¿ã€‚\n'
        '4. å¯¹äºå…·ä½“çš„è®¾å¤‡å‚æ•°ï¼ˆå¦‚å°ºå¯¸ã€ç”µæµï¼‰ï¼Œå¦‚æœä½ åœ¨å†å²æƒ…æŠ¥ä¸­æ²¡çœ‹åˆ°ï¼Œ'
        'è¯·ä½¿ç”¨ [éœ€å¡«å…¥æˆ‘æ–¹å…·ä½“å‚æ•°] å ä½ï¼Œä¸è¦çç¼–ã€‚\n'
        '5. è¯­è¨€ç®€æ´ä¸“ä¸šï¼Œé€‚åˆå·¥ç¨‹å¸ˆé˜…è¯»ã€‚'
    ),
}


def generate_tech_summary(api_key: str, context_data: str,
                          channel: str = "email",
                          tech_competitor: str = "",
                          tech_status: str = "",
                          tech_pain_points: list = None,
                          tech_role: list = None) -> str:
    """åŸºäºé¡¹ç›®æƒ…æŠ¥ + å››ç»´é…ç½®ï¼Œç”Ÿæˆ Miller Heiman ä½“ç³»çš„æŠ€æœ¯ä¸å•†åŠ¡èåˆæ–¹æ¡ˆæ‘˜è¦ã€‚"""
    client = OpenAI(api_key=api_key)

    pain_points_str = "ã€".join(tech_pain_points) if tech_pain_points else "æœªæ˜ç¡®å…·ä½“ç—›ç‚¹"
    role_str = "ã€".join(tech_role) if tech_role else "æœªæŒ‡å®š"

    prompt = (
        f"ä½ æ˜¯ä¸€åæ·±è°™å¤æ‚å¤§å®¢æˆ·é”€å”®ï¼ˆMiller Heimanä½“ç³»ï¼‰çš„é¡¶çº§å·¥ä¸šç”µæ°”æ€»å·¥ç¨‹å¸ˆã€‚\n"
        f"è¯·ä¸ºå½“å‰é¡¹ç›®çš„ç‰¹å®šå—ä¼—ï¼Œç”Ÿæˆä¸€ä»½æå…·æ€ä¼¤åŠ›çš„ã€æŠ€æœ¯ä¸å•†åŠ¡èåˆæ–¹æ¡ˆæ‘˜è¦ã€‘ã€‚\n\n"

        f"ã€ğŸ¯ å››ç»´åˆ¶å¯¼å‚æ•°ã€‘ï¼š\n"
        f"1. æ²Ÿé€šå¯¹è±¡èº«å…¼çš„è§’è‰²ï¼š{role_str}\n"
        f"2. æ˜ç¡®çš„å¯¹æ¯”å‹å•†ï¼š{tech_competitor if tech_competitor.strip() else 'æœªæŒ‡å®š'}"
        f" (å¦‚æœæŒ‡å®šäº†å‹å•†ï¼Œè¯·é‡‡å–ä¸“ä¸šä¸”éšè”½çš„æ‹‰è¸©ç­–ç•¥ï¼Œå¼ºè°ƒæˆ‘ä»¬çš„å·®å¼‚åŒ–ä¼˜åŠ¿)\n"
        f"3. å®¢æˆ·å½“å‰ç³»ç»Ÿç°çŠ¶ï¼š{tech_status if tech_status.strip() else 'æœªæä¾›'}\n"
        f"4. å®¢æˆ·æ ¸å¿ƒç—›ç‚¹ï¼š{pain_points_str}\n\n"

        f"ã€è¾“å‡ºç»å¯¹çº¢çº¿ã€‘ï¼š\n"
        f"1. å¿…é¡»å®Œå…¨è´´åˆã€{role_str}ã€‘çš„å¤åˆè§†è§’ï¼"
        f"å¦‚æœä»–æ—¢æ˜¯å†³ç­–è€…åˆæ˜¯å½±å“è€…ï¼Œä½ æ—¢è¦è®²ROIä¹Ÿè¦è®²æŠ€æœ¯åˆè§„ï¼›"
        f"å¦‚æœæ˜¯ç»™ã€Œæ•™ç»ƒ/å†…çº¿ã€ï¼Œè¦æä¾›èƒ½ç›´æ¥å¤åˆ¶ç”¨äºå‘ä¸Šçº§æ±‡æŠ¥çš„æ§æ ‡ç†ç”±ã€‚\n"
        f"2. é’ˆå¯¹ã€æ ¸å¿ƒç—›ç‚¹ã€‘ï¼Œå¿…é¡»æå‡ºæˆ‘ä»¬æ–¹æ¡ˆä¸­å¯¹åº”çš„"
        f"ã€Œç‰¹å¾(Feature) + ä¼˜åŠ¿(Advantage) + åˆ©ç›Š(Benefit)ã€ï¼\n"
        f"3. è¾“å‡º 3-4 ä¸ªæ ¸å¿ƒæ®µè½ï¼Œæ‹’ç»ç½—åˆ—çŸ­å¥ï¼Œæ‹’ç»å‡å¤§ç©ºçš„åºŸè¯ã€‚"
        f"å¿…é¡»åƒä¸€ä»½ä¸“ä¸šçš„ã€å¯ç›´æ¥å¾®ä¿¡è½¬å‘ç»™è¯¥è§’è‰²çš„æ­£å¼æ±‡æŠ¥æ‘˜è¦ã€‚\n"
        f"4. é‡åˆ°ç¼ºå¤±çš„å…·ä½“è®¾å¤‡å‚æ•°ï¼Œä¸¥æ ¼ä½¿ç”¨ [éœ€å¡«å…¥å…·ä½“å‚æ•°] å ä½ã€‚"
    )

    # å¾®ä¿¡æ¸ é“è¿½åŠ ç§»åŠ¨ç«¯æ ¼å¼çº¦æŸ
    if channel == "wechat":
        prompt += (
            "\n5. å› ä¸ºæœ€ç»ˆé€šè¿‡å¾®ä¿¡å‘é€ï¼Œæ¯ä¸ªæ®µè½æ§åˆ¶åœ¨ 3-5 è¡Œä¹‹å†…ï¼Œ"
            "é€‚å½“ä½¿ç”¨ âœ… ğŸ“Š ğŸ”§ ç­‰ç¬¦å·å¢å¼ºå¯è¯»æ€§ã€‚"
        )

    response = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[
            {"role": "system", "content": prompt},
            {"role": "user", "content": f"ã€é¡¹ç›®å†å²æƒ…æŠ¥ã€‘\n{context_data}"},
        ],
        temperature=0.5,
    )

    return response.choices[0].message.content


def generate_insider_ammo(api_key: str, context_data: str,
                          channel: str = "wechat",
                          target_person: str = "æ•™ç»ƒ/å†…çº¿",
                          project_stage: str = "åˆæœŸæ¥è§¦",
                          leader_attitude: str = "",
                          leader_history: str = "") -> str:
    """ä¸ºæ•™ç»ƒ/å†…çº¿ä¸€æ¬¡æ€§ç”Ÿæˆ 3 ç§ä¾§é‡ç‚¹çš„å‘ä¸Šç®¡ç†è¯æœ¯ã€‚"""
    client = OpenAI(api_key=api_key)

    prompt = (
        f"ä½ æ˜¯ä¸€åæ·±è°™å¤æ‚é”€å”®åšå¼ˆå’ŒèŒåœºå‘ä¸Šç®¡ç†çš„é¡¶çº§å†›å¸ˆã€‚\n"
        f"è¯·æ ¹æ®ä»¥ä¸‹é¡¹ç›®æƒ…æŠ¥ï¼Œä¸ºæˆ‘ä»¬çš„ã€Œå†…çº¿ï¼ˆæ•™ç»ƒï¼‰ã€å†™å‡º 3 ä¸ªä¸åŒä¾§é‡ç‚¹çš„ã€å†…éƒ¨æ±‡æŠ¥è¯æœ¯ã€‘ã€‚\n\n"

        f"ã€ğŸ¯ ç»ˆæé¶å‘ï¼šé¢†å¯¼å¿ƒç†ç”»åƒã€‘ï¼š\n"
        f"ä½ è¦æ±‡æŠ¥çš„è¿™ä½æœ€é«˜å†³ç­–è€…ï¼Œç›®å‰çš„å¿ƒç†çŠ¶æ€æ˜¯ï¼šã€{leader_attitude if leader_attitude else 'æœªæŒ‡å®š'}ã€‘ã€‚\n"
        f"ä»–ç›®å‰çš„èƒŒæ™¯è½¨è¿¹æ˜¯ï¼šã€{leader_history if leader_history else 'æœªæŒ‡å®š'}ã€‘ã€‚\n"
        f"ä½ çš„è¯æœ¯å¿…é¡»**æå…¶ç²¾å‡†åœ°åˆ‡ä¸­/åˆ©ç”¨ä»–ç°åœ¨çš„è¿™ä¸ªå¿ƒç†è½¯è‚‹æˆ–ä¿¡ä»»åŸºç¡€**ï¼\n\n"

        f"ã€æ ¸å¿ƒåŸåˆ™ã€‘ï¼š\n"
        f"1. ç»å¯¹ä¼ªè£…ï¼šè¯­æ°”å¿…é¡»å®Œå…¨æ˜¯ã€Œå®¢æˆ·å†…éƒ¨æŠ€æœ¯è´Ÿè´£äºº/å‘˜å·¥ã€ï¼Œ"
        f"ç”¨ã€Œé‚£å®¶å…¨å¯†å°çš„å‚å®¶ã€ç§°å‘¼æˆ‘ä»¬ã€‚\n"
        f"2. æå…¶å£è¯­åŒ–ï¼šåƒåœ¨å¾®ä¿¡é‡Œç»™è€æ¿å‘è¯­éŸ³ï¼Œ"
        f"åŒ…è£…æˆã€Œä¸ºå…¬å¸ç”Ÿäº§å®‰å…¨/ä¸ºè€æ¿é’±åŒ…ç€æƒ³ã€ã€‚\n"
        f"3. å¿ƒç†æŒ‰æ‘©ï¼šå¦‚æœä»–æ€•æ‹…è´£ï¼Œå°±å¼ºè°ƒæˆ‘ä»¬æ–¹æ¡ˆçš„å…œåº•èƒ½åŠ›ï¼›"
        f"å¦‚æœä»–æŠ é—¨ï¼Œå°±å¼ºè°ƒéšæ€§æˆæœ¬ï¼›"
        f"å¦‚æœä»–æœ‰å¿ƒç†é˜´å½±ï¼Œå°±åˆ©ç”¨é‚£ä¸ªé˜´å½±å»æ‰“å‹ç«å“ã€‚\n"
        f"4. é‡åˆ°ç¼ºå¤±çš„å…·ä½“å‚æ•°ï¼Œä¸¥æ ¼ä½¿ç”¨ [éœ€å¡«å…¥å…·ä½“å‚æ•°] å ä½ã€‚\n\n"

        f"ã€å¿…é¡»è¾“å‡ºä»¥ä¸‹ä¸‰ä¸ªç‰ˆæœ¬ã€‘ï¼š\n"
        f"ğŸ¯ ç‰ˆæœ¬ä¸€ï¼šã€ç—›é™ˆåˆ©å®³æ´¾ã€‘ï¼ˆä¾§é‡ï¼šåœæœºé£é™©ä¸ç”Ÿäº§å®‰å…¨ï¼‰\n"
        f"ğŸ¯ ç‰ˆæœ¬äºŒï¼šã€å·æ¢æ¦‚å¿µæ´¾ã€‘ï¼ˆä¾§é‡ï¼šæš—ä¸­æ§æ ‡ä¸è®¾å®šé—¨æ§›ï¼‰\n"
        f"ğŸ¯ ç‰ˆæœ¬ä¸‰ï¼šã€ç®—æ€»è´¦æ´¾ã€‘ï¼ˆä¾§é‡ï¼šå…¨ç”Ÿå‘½å‘¨æœŸæˆæœ¬ TCOï¼‰\n\n"

        f"è¯·ç”¨æ¸…æ™°çš„ Markdown æ ¼å¼åˆ†ç‚¹è¾“å‡ºè¿™ä¸‰ä¸ªç‰ˆæœ¬ï¼Œå­—æ•°è¦ç²¾ç‚¼ï¼"
    )

    response = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[
            {"role": "system", "content": prompt},
            {"role": "user", "content": f"ã€é¡¹ç›®å†å²æƒ…æŠ¥ã€‘\n{context_data}"},
        ],
        temperature=0.75,
    )

    return response.choices[0].message.content


# â”€â”€ æ²™ç›˜è¯æœ¯ç”Ÿæˆï¼ˆæ·±åº¦æ‰“ç£¨ç‰ˆï¼‰ â”€â”€

# æœ€é«˜æŒ‡ä»¤ï¼šç„Šæ­»åœ¨æ¯æ¬¡è¯·æ±‚æœ€å‰é¢çš„ System Persona
_SALES_PERSONA = (
    "ã€æœ€é«˜æŒ‡ä»¤ â€” ä½ çš„èº«ä»½ã€‘\n"
    "ä½ æ˜¯ä¸€ä¸ªæ‹¥æœ‰ 15 å¹´ B2B å¤§å®¢æˆ·é”€å”®ç»éªŒçš„é¡¶çº§é”€å† ã€‚\n"
    "æ·±è°™äººæ€§ã€æ‡‚æƒè°‹ã€æ‡‚åˆ©ç›Šäº¤æ¢ï¼Œè¯´è¯ä¸€é’ˆè§è¡€ã€‚\n"
    "ä½ çš„ä¸€åˆ‡è¾“å‡ºå¿…é¡»åŸºäºçœŸå®å•†æˆ˜é€»è¾‘â€”â€”ä¸è¯´åºŸè¯ã€ä¸ç”¨å¥—è¯æ¨¡æ¿ã€\n"
    "ä¸æ'å°Šæ•¬çš„Xæ€»æ‚¨å¥½'å¼å®˜è…”ã€‚\n"
    "ä½ çš„ç›®æ ‡æ˜¯ç”¨æœ€è‡ªç„¶ã€æœ€é«˜æƒ…å•†çš„è¯­è¨€æ¨è¿›é¡¹ç›®ï¼Œæ‰«é™¤åºŸæ ‡é£é™©ã€‚\n\n"
)

_PITCH_PROMPTS = {
    "wechat_msg": (
        "ã€ä»»åŠ¡ï¼šå¾®ä¿¡è·Ÿè¿›æ¶ˆæ¯ã€‘\n"
        "ä½ ç°åœ¨è¦ç»™å®¢æˆ·å…³é”®äººå‘ä¸€æ¡å¾®ä¿¡ã€‚ä¸¥æ ¼éµå®ˆï¼š\n"
        "1. æ€»å­—æ•° â‰¤ 150 å­—ï¼åƒçœŸäººèŠå¤©ï¼Œåˆ«é•¿ç¯‡å¤§è®ºã€‚\n"
        "2. å¼€å¤´ç”¨ä¸€å¥ç”Ÿæ´»åŒ–å¯’æš„ç ´å†°ï¼ˆå¤©æ°”/è¡Œä¸šæ–°é—»/å¯¹æ–¹è¿‘å†µï¼‰ï¼Œä½†ä¸è¶…è¿‡ 20 å­—ã€‚\n"
        "3. æ ¸å¿ƒï¼šé’ˆå¯¹ä¸‹æ–¹ã€åºŸæ ‡é£é™©/æƒ…æŠ¥ç›²åŒºã€‘ä¸­æœ€è‡´å‘½çš„ä¸€æ¡ï¼Œ\n"
        "   å·§å¦™åœ°æŠ›å‡ºä¸€ä¸ª'è¯±é¥µ'â€”â€”æ¯”å¦‚ï¼š'æˆ‘è¿™è¾¹åˆšæ‹¿åˆ°ä¸€ä»½å¯¹æ ‡æ•°æ®ï¼Œ\n"
        "   è·Ÿä½ ä»¬ç°åœ¨é€‰å‹æ–¹å‘é«˜åº¦ç›¸å…³ï¼Œæ‰¾ä¸ªæ—¶é—´ç»™æ‚¨å½“é¢æ‹†è§£ä¸€ä¸‹ï¼Ÿ'\n"
        "4. å¦‚æœä¸‹æ–¹æœ‰ã€ç«å“æƒ…æŠ¥ã€‘ï¼Œå¿…é¡»ä¾§é¢æ•²æ‰“ä¸€ä¸‹å®¢æˆ·â€”â€”\n"
        "   ç”¨æš—ç¤ºè€Œéç›´æ¥æ”»å‡»ï¼ˆä¾‹å¦‚ï¼š'æœ€è¿‘å¬è¯´æœ‰äº›å‹å•†äº¤æœŸæ‰¿è¯ºå¾ˆæ¿€è¿›ï¼Œ\n"
        "   åˆ°æ—¶å€™è½åœ°å¯èƒ½æœ‰gap...'ï¼‰ã€‚\n"
        "5. ç»“å°¾å¿…é¡»ç•™ä¸€ä¸ªè½»æ¾çš„äº’åŠ¨é’©å­ï¼Œå¼•å¯¼å¯¹æ–¹å›å¤ã€‚\n"
        "6. é€‚å½“ä½¿ç”¨ emojiï¼ŒåƒçœŸäººå¾®ä¿¡ï¼Œä¸è¦æ„Ÿå¹å·æ»¡å¤©é£ã€‚\n"
        "7. ç»å¯¹ç¦æ­¢ï¼š'å°Šæ•¬çš„'ã€'è´µå¸'ã€'ä¸èƒœè£å¹¸'ç­‰ç¿»è¯‘è…”ã€‚\n"
    ),
    "email": (
        "ã€ä»»åŠ¡ï¼šæ­£å¼å•†åŠ¡è·Ÿè¿›é‚®ä»¶ã€‘\n"
        "ä½ ç°åœ¨è¦å†™ä¸€å°å¯ä»¥ç›´æ¥å‘é€çš„å•†åŠ¡é‚®ä»¶ã€‚ä¸¥æ ¼éµå®ˆï¼š\n"
        "1. å¿…é¡»åŒ…å«ï¼šé‚®ä»¶ä¸»é¢˜è¡Œ + æ­£æ–‡ + è½æ¬¾ã€‚\n"
        "2. è¯­æ°”ï¼šä¸å‘ä¸äº¢ï¼Œä¸“ä¸šä½†ä¸å†°å†·ï¼Œåƒä¸€ä¸ªè‡ªä¿¡çš„è¡Œä¸šä¸“å®¶ã€‚\n"
        "3. æ ¸å¿ƒç­–ç•¥â€”â€”æŠŠä¸‹æ–¹ã€æ§æ ‡ç‚¹ã€‘åŒ…è£…æˆ'å¯¹å®¢æˆ·é¡¹ç›®æå…¶è´Ÿè´£çš„æŠ€æœ¯å»ºè®®'ï¼š\n"
        "   ä¾‹å¦‚æ§æ ‡ç‚¹æ˜¯'é¢„ç®—æœªç¡®è®¤'ï¼Œä½ è¦å†™æˆ'ä¸ºç¡®ä¿è´µæ–¹é¡¹ç›®é¡ºåˆ©æ¨è¿›ï¼Œ\n"
        "   æˆ‘ä»¬å»ºè®®åœ¨æ‹›æ ‡å‰å®Œæˆé¢„ç®—æ¡†æ¶çš„åˆæ­¥å¯¹é½ï¼Œä»¥ä¸‹æ˜¯æˆ‘æ–¹çš„å‚è€ƒæ–¹æ¡ˆ...'ã€‚\n"
        "4. ç«å“æŒ–å‘ï¼šä»ã€ç«å“æƒ…æŠ¥ã€‘ä¸­æ‰¾åˆ°å¯¹æ‰‹çš„å¼±ç‚¹ï¼Œ\n"
        "   ä¸ç›´æ¥ç‚¹åæ”»å‡»ï¼Œè€Œæ˜¯åœ¨é‚®ä»¶ä¸­è‡ªç„¶åœ°æå‡ºä¸€ä¸ª'è¯„ä¼°ç»´åº¦'ï¼Œ\n"
        "   è®©å®¢æˆ·åœ¨å¯¹æ¯”æ—¶è‡ªå·±å‘ç°ç«å“çš„çŸ­æ¿ã€‚\n"
        "5. ç»“å°¾å¿…é¡»ç»™å‡ºä¸€ä¸ªæ˜ç¡®çš„ Call-to-Actionï¼ˆçº¦ä¼šè®®/é€æ ·/æŠ€æœ¯äº¤æµï¼‰ã€‚\n"
        "6. ç»å¯¹ç¦æ­¢ï¼šç©ºæ´çš„è‡ªå–è‡ªå¤¸ã€æ— ä¿¡æ¯é‡çš„å¥—è¯ã€‚\n"
    ),
    "internal_strategy": (
        "ã€ä»»åŠ¡ï¼šå†…éƒ¨æ”»é˜²ä½œæˆ˜æ–¹æ¡ˆã€‘\n"
        "ä½ ç°åœ¨è¦ä¸ºé”€å”®å›¢é˜Ÿè¾“å‡ºä¸€ä»½å†·é…·ã€ç²¾å‡†çš„å†…éƒ¨ä½œæˆ˜æ–¹æ¡ˆã€‚\n"
        "è¿™ä¸æ˜¯ç»™å®¢æˆ·çœ‹çš„ï¼Œæ˜¯ç»™è‡ªå·±äººçœ‹çš„ã€‚ç”¨è¯è¦ç‹ ã€å‡†ã€å†·é…·ã€‚\n"
        "å¿…é¡»åŒ…å«ä»¥ä¸‹å››ä¸ªæ¿å—ï¼ˆMarkdown æ ¼å¼ï¼‰ï¼š\n\n"
        "## ğŸ¯ é¡¹ç›®æ­»ç©´è¯Šæ–­\n"
        "ç›´æ¥åˆ—å‡ºå½“å‰é¡¹ç›®æœ€è‡´å‘½çš„ 2-3 ä¸ªé£é™©ç‚¹ï¼Œ\n"
        "åŸºäºä¸‹æ–¹çš„ã€åºŸæ ‡é£é™©ã€‘å’Œã€æ§æ ‡ç‚¹ã€‘ï¼Œä¸€é’ˆè§è¡€ç‚¹å‡ºå“ªé‡Œå¯èƒ½ç¿»è½¦ã€‚\n\n"
        "## âš”ï¸ ç«å“æ”»å‡»è·¯çº¿\n"
        "åŸºäºã€ç«å“æƒ…æŠ¥ã€‘åˆ†æï¼šå¯¹æ‰‹çš„å¼±ç‚¹åœ¨å“ªï¼Ÿ\n"
        "å“ªä¸ªå†³ç­–äººå¯èƒ½å·²è¢«ç«å“æå®šï¼Ÿæˆ‘ä»¬æ€ä¹ˆåæ”»ï¼Ÿ\n"
        "ç»™å‡ºå…·ä½“çš„æ‹‰è¸©è¯æœ¯å’Œå·®å¼‚åŒ–æ‰“æ³•ã€‚\n\n"
        "## ğŸ›¡ï¸ é£é™©è§„é¿ & æ§æ ‡å»ºè®®\n"
        "é’ˆå¯¹æ¯ä¸€æ¡åºŸæ ‡é£é™©ï¼Œç»™å‡ºå…·ä½“çš„è§„é¿æªæ–½ï¼š\n"
        "è¦è¡¥å“ªäº›ææ–™ï¼Ÿè¦çº¦è§è°ï¼Ÿè¦æ€ä¹ˆè®¾ç½®è¯„æ ‡é—¨æ§›è®©æˆ‘æ–¹å ä¼˜ï¼Ÿ\n\n"
        "## â° 48 å°æ—¶å…³é”®æ¨è¿›åŠ¨ä½œ\n"
        "åˆ—å‡º 3-5 ä»¶å¿…é¡»åœ¨ 48 å°æ—¶å†…å®Œæˆçš„å…·ä½“åŠ¨ä½œï¼Œ\n"
        "åŒ…æ‹¬ï¼šçº¦è§å“ªä¸ªå…³é”®äººã€é€ä»€ä¹ˆèµ„æ–™ã€å®‰æ’ä»€ä¹ˆå±€ã€‚\n"
        "å¦‚æœéœ€è¦è¯·å®¢åƒé¥­æˆ–åˆ©ç›Šç»‘å®šï¼Œç›´è¯´ï¼è¿™æ˜¯å†…éƒ¨æ–‡æ¡£ã€‚\n"
    ),
    "tech_solution": (
        "ã€ä»»åŠ¡ï¼šæŠ€æœ¯æ–¹æ¡ˆæ‘˜è¦ã€‘\n"
        "è¾“å‡ºä¸€ä»½å¯ç›´æ¥å‘ç»™å®¢æˆ·æŠ€æœ¯å›¢é˜Ÿçš„æŠ€æœ¯æ–¹æ¡ˆæ‘˜è¦ã€‚ä¸¥æ ¼éµå®ˆï¼š\n"
        "1. å¼ºåˆ¶ä½¿ç”¨ FAB ç»“æ„ï¼šFeature(ç‰¹å¾) â†’ Advantage(ä¼˜åŠ¿) â†’ Benefit(åˆ©ç›Š)ã€‚\n"
        "2. é’ˆå¯¹ä¸‹æ–¹ã€æ§æ ‡ç‚¹ã€‘è®¾ç½®å¯¹æˆ‘æ–¹æœ‰åˆ©çš„è¯„æ ‡ç»´åº¦å’ŒæŠ€æœ¯é—¨æ§›ã€‚\n"
        "3. ç”¨'è¡Œä¸šå¸¸è§é£é™©'åŒ…è£…ç«å“å¼±ç‚¹ï¼Œä¸ç›´æ¥ç‚¹åæ”»å‡»ã€‚\n"
        "4. ç¼ºå¤±çš„å…·ä½“å‚æ•°ï¼Œä¸¥æ ¼ä½¿ç”¨ [éœ€å¡«å…¥å…·ä½“å‚æ•°] å ä½ã€‚\n"
        "5. ç”¨ Markdown æ ¼å¼è¾“å‡º 3-4 ä¸ªæ ¸å¿ƒæ®µè½ï¼Œæå…¶ä¸¥è°¨ä¸“ä¸šã€‚\n"
        "6. åƒä¸€ä»½å¯ç›´æ¥å¾®ä¿¡è½¬å‘ç»™æŠ€æœ¯è´Ÿè´£äººçš„æ­£å¼æ±‡æŠ¥æ–‡æ¡£ã€‚\n"
    ),
}

_PITCH_TEMPERATURES = {
    "wechat_msg": 0.7,
    "email": 0.6,
    "internal_strategy": 0.5,
    "tech_solution": 0.4,
}

# â”€â”€ è§’è‰²é¶å‘ç²¾å‡†æ‰“å‡»ç­–ç•¥ â”€â”€

_ROLE_STRATEGIES = {
    "å†³ç­–è€…": (
        "\nã€ğŸ¯ è§’è‰²é¶å‘ï¼šå†³ç­–è€…ã€‘\n"
        "è¯æœ¯å¿…é¡»æ‹”é«˜ï¼æ ¸å¿ƒå…³æ³¨ï¼š\n"
        "- ROIï¼ˆæŠ•èµ„å›æŠ¥ç‡ï¼‰ã€é™æœ¬å¢æ•ˆã€ä¸šåŠ¡å®‰å…¨ä¸æ”¿ç»©é¢å­\n"
        "- å¸®ä»–è§„é¿æœ€å¤§çš„é›·åŒºï¼Œç”¨æ•°æ®å’Œå¤§å±€è§‚å¾æœä»–\n"
        "è¯­æ°”ï¼šè‡ªä¿¡ã€æœ‰åˆ†é‡ã€åƒä¸€ä¸ªå€¼å¾—ä¿¡èµ–çš„è¡Œä¸šé¡¾é—®ã€‚\n"
    ),
    "ä½¿ç”¨è€…": (
        "\nã€ğŸ› ï¸ è§’è‰²é¶å‘ï¼šä½¿ç”¨è€…ã€‘\n"
        "è¯æœ¯è¦æ¥åœ°æ°”ï¼æ ¸å¿ƒå…³æ³¨ï¼š\n"
        "- ç³»ç»Ÿç¨³å®šæ€§ã€æ“ä½œä¾¿æ·æ€§ã€å”®åæœåŠ¡å“åº”é€Ÿåº¦\n"
        "- è®©ä»–ç¡®ä¿¡ç”¨æˆ‘ä»¬çš„æ–¹æ¡ˆ'å¥½å¹²æ´»ã€ä¸èƒŒé”…'\n"
        "è¯­æ°”ï¼šåŠ¡å®ã€è´´å¿ƒã€åƒä¸€ä¸ªé è°±çš„æŠ€æœ¯è€å‹ã€‚\n"
    ),
    "å½±å“è€…": (
        "\nã€âš–ï¸ è§’è‰²é¶å‘ï¼šå½±å“è€…ã€‘\n"
        "è¯æœ¯è¦ä½“ç°ä¸“ä¸šå‹åˆ¶ï¼æ ¸å¿ƒå…³æ³¨ï¼š\n"
        "- å‚æ•°å£å’ã€åˆè§„æ€§ã€æŠ€æœ¯å…ˆè¿›æ€§\n"
        "- ç”¨æˆ‘ä»¬çš„'æ§æ ‡ç‚¹'ç»™ä»–æä¾›æ‰“å‡»ç«å“çš„æ­¦å™¨å¼¹è¯\n"
        "è¯­æ°”ï¼šä¸¥è°¨ã€ä¸“ä¸šã€å……æ»¡æŠ€æœ¯ä¼˜è¶Šæ„Ÿã€‚\n"
    ),
    "æ•™ç»ƒ/å†…çº¿": (
        "\nã€ğŸ•µï¸ è§’è‰²é¶å‘ï¼šæ•™ç»ƒ/å†…çº¿ã€‘\n"
        "è¯æœ¯è¦åƒè‡ªå·±äººï¼æ ¸å¿ƒå…³æ³¨ï¼š\n"
        "- å†…éƒ¨æ”¿æ²»æ ¼å±€ã€ä¸ªäººç§äº¤ã€åˆ©ç›Šç»‘å®š\n"
        "- æä¾›èƒ½è®©ä»–å»å‘ä¸Šçº§é‚€åŠŸçš„æ§æ ‡ç´ æï¼Œæˆ–åˆºæ¢ç«å“çš„è‡´å‘½æƒ…æŠ¥\n"
        "è¯­æ°”ï¼šæå…¶äº²å¯†ã€å£è¯­åŒ–ã€åƒå¾®ä¿¡ç§èŠå…„å¼Ÿ/é—ºèœœã€‚\n"
    ),
}


def generate_sales_pitch(api_key: str, context_data: str,
                         pitch_type: str = "wechat_msg",
                         target_role: str = "",
                         llm_configs: dict | None = None) -> str:
    """
    åŸºäºé¡¹ç›®æ²™ç›˜æƒ…æŠ¥åŠ¨æ€ç”Ÿæˆå®æˆ˜è¯æœ¯ã€‚
    pitch_type: wechat_msg | email | internal_strategy | tech_solution
    target_role: å†³ç­–è€… | ä½¿ç”¨è€… | å½±å“è€… | æ•™ç»ƒ/å†…çº¿ | ""(ä¸é™å®š)
    context_data: åºåˆ—åŒ–åçš„é¡¹ç›®å…¨é‡æƒ…æŠ¥æ–‡æœ¬ï¼ˆå«ä¼˜å…ˆçº§é“¾æ³¨å…¥ï¼‰ã€‚
    llm_configs: å‰ç«¯ä¼ å…¥çš„åŠ¨æ€ LLM è·¯ç”±é…ç½®ï¼ˆå¯é€‰ï¼‰ã€‚
    """
    # ç„Šæ­» persona + ä»»åŠ¡æŒ‡ä»¤
    system_prompt = _SALES_PERSONA + _PITCH_PROMPTS.get(
        pitch_type, _PITCH_PROMPTS["wechat_msg"]
    )

    # è§’è‰²é¶å‘æ³¨å…¥ï¼ˆç¬¬äºŒé¡ºä½ï¼‰
    if target_role and target_role in _ROLE_STRATEGIES:
        system_prompt += _ROLE_STRATEGIES[target_role]

    temperature = _PITCH_TEMPERATURES.get(pitch_type, 0.6)

    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": f"ã€é¡¹ç›®æ²™ç›˜æƒ…æŠ¥ â€” å…¨é‡æ³¨å…¥ã€‘\n{context_data}"},
    ]

    # ä½¿ç”¨ GlobalLLMRouter é«˜å¯ç”¨è·¯ç”±
    router = build_llm_router(primary_api_key=api_key, llm_configs=llm_configs)
    return router.chat(messages=messages, temperature=temperature)


def transcribe_audio(api_key: str, audio_bytes: bytes) -> str:
    """ä½¿ç”¨ OpenAI Whisper API å°†éŸ³é¢‘è½¬ä¸ºæ–‡å­—ã€‚"""
    import tempfile, os
    client = OpenAI(api_key=api_key)

    # å†™å…¥ä¸´æ—¶æ–‡ä»¶ä¾› API è¯»å–
    tmp = tempfile.NamedTemporaryFile(suffix=".wav", delete=False)
    try:
        tmp.write(audio_bytes)
        tmp.close()
        with open(tmp.name, "rb") as audio_file:
            transcript = client.audio.transcriptions.create(
                model="whisper-1",
                file=audio_file,
                language="zh",
            )
        return transcript.text
    finally:
        os.unlink(tmp.name)
